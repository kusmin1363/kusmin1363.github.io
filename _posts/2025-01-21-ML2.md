---
layout: single
title:  "2일차. Bayesian Decision Theory"
---

## 목차

1. [확률의 종류](#1-확률의-종류)
2. [Bayesian Classification](#2-Bayesian-Classification)
3. [그래서 classification은 어떻게 하는건가](#3-그래서-classification은-어떻게-하는건가)

### 1. 확률의 종류
- Prior, Posterior, Joint, Marginal
1. Prior = 사전 확률 : 특정 사건이 일어나기 전의 확률. 기초 정보를 기반으로 하는 확률
2. Posterior = 사후 확률 : 특정 사건이 일어나고 나서, 다른 사건이 일어날 확률. 기초 정보 + 새로운 정보가 들어왔을 때의 확률
3. Joint = 2개의 사건이 동시에 일어날 확률
4. Marginal = 개별 사건이 일어날 확률
- Bayes' Theorem = 조건부확률을 계산하기 위해 사용됨

$$ P(a|b) = \frac{P(a,b)}{P(b)} $$
$$ P(a|b)P(b) = P(a,b) $$

### 2. Bayesian Classification
- 조건부 확률을 기반으로 입력 x가 어떤 class에 속할지를 분류함
- x가 Class 1인지, Class 2인지 모르겠다면 P(c=1|x)와 P(c=2|x)를 비교
- 그렇다면 P(c=1|x), P(c=2|x)는 어떻게 계산하는가? => Bayes' Theorem 도입

![Alt Text](assets/2025-01-21-ML2/image.jpg)

P(c|x)는 몰라도 P(x|c), P(c), P(x)를 계산함으로써, P(c|x)도 계산 가능
1. P(x|c) = Likelihood라고 부름. 특정 클래스에 x가 존재할 가능성
    - P(x|c=1)이면, Class 1에서 x가 존재할 가능성
2. P(c) = prior라고 부름. 앞서 소개했듯, 기초 정보를 가지고 class 전체 중 특정 클래스의 비율을 뜻함.
3. P(x) = evidence라고 부름. 전체 데이터에서 x라는 데이터가 목격될 확률

### 3. 그래서 classification은 어떻게 하는건가
- $P(c_i | x)$ 가 제일 클 떄의 i로 분류함

$$ \underset{i}{\arg\max}P(c_i|x) = \underset{i}{\arg\max}\frac{P(x|c_i)P(c_i)}{P(x)} = \underset{i}{\arg\max}P(x|c_i)P(c_i) = \underset{i}{\arg\max}[logP(x|c_i) + logP(c_i)]$$

$P(x)$ 가 공통 분모이기 때문에 소거

$log$를 취해도 $\underset{i}{\arg\max}$ 에는 여향이 없기 때문에 $log$ 까지

### 4. Bayesian Decision Rule
- 단순히 클래스에 속할 가능성을 놓고 비교하는 Bayesian Classifiaction에서 좀 더 발전된 분류 규칙
- 가능성 뿐만 아니라, "분류했을 때 얼마나 손해보는가"까지도 같이 비교

* Expected Risk(예상 손해)
    * $\alpha_i$ = 입력을 $c_i$로 분류한다는 행위
    * $\lambda_{ik}$ = 실제로 $c_k$ 인 입력을 $\alpha_i$ 함으로써 생기는 손해
    * $R(\alpha_i|x)$ = 입력을 $c_i$ 로 분류했을 때의 위험부담 $\equiv \sum_k \lambda_{ik}P(c_k|x)$ 
        
        = (입력이 i를 제외한 class일 확률) $\times$ (그 상황에서의 손해) 의 총합

- 즉, Bayesian Decision Rule은 $i = \underset{i}{\arg\max}R(\alpha_i|x)$ 인 $i$를 골라 $\alpha_i$ 하는 것
