---
layout: single
title:  "DL-1. 딥러닝 모델 만들기"
---


# 딥러닝 모델 만들기

## 1. Torch.nn 소개
- Pytorch의 핵심 모듈, 딥러닝을 구축하기 위한 기초
- 기능 : Layer, Activation Function, Loss Function, Optimizer 등을 설정 가능
- Layer = 딥러닝의 Layer를 설정 가능. Linear, Convolutional, MaxPooling 등 다양한 Layer로 설정 가능
- Activation Function = NN에 비선형성을 도입해주는 함수. ReLu, Sigmoid, Tanh 등으로 설정 가능
- Loss Function = 모델이 최소화하고자 하는 오류. 오류를 측정하는 방식을 MSELoss, CrossEntropyLoss 등으로 설정 가능
- Optimizer = 매개변수 조정과정에서 어떤 방식으로 최적화를 시키는가. SGD, Adam, RmSprop 등 다양한 최적화 알고리즘



```python
import torch
import torch.nn as nn
```

## 2. Layer 설정 방법
- input layer, hidden layer, output layer로 크게 3개로 분류될 수 있다.
- torch.nn은 이 3가지를 한 번에 설정할 수 있다. 함수를 이용해 hidden layer의 종류를 정할 수 있으며, input_feature, output_feature는 공통적으로 설정할 수 있다.
- nn.Linear, nn.Conv2d, nn.MaxPoold, nn.LSTM
- 이거는 단순히 1층짜리 Layer를 만드는 방법이고, Multilayer로 만들려면 Layer끼리 연결해야 한다.


```python
# in_features: 입력 특성(또는 차원)의 수입니다.
# out_features: 출력 특성(또는 차원)의 수입니다.
# bias: 편향을 사용할지 여부를 결정하는 부울 값입니다. 기본값은 True이며, 이 경우 학습 가능한 편향이 추가됩니다.

linear_layer = nn.Linear(in_features=10, out_features=5, bias=True)

# 임의의 데이터 생성
input_tensor = torch.randn(1, 10)

# 생성한 Linear 레이어에 입력 데이터를 전달하여 출력 데이터를 얻음
# linear_layer의 설정에 따라 Tensor의 크기가 10에서 5로 줄어든다.
output_tensor = linear_layer(input_tensor)

```

### Convolution Layer 설정
1. in_channels: 입력 채널의 수 => m * n 짜리 data가 몇장인가?
2. out_channels: 출력 채널의 수 => 새롭게 x * y짜리 data가 몇장인가?
3. kernel_size: 커널(필터)의 크기. 정수 혹은 (높이, 너비) 형태의 튜플 => m * n 을 어떤 kernel로 계산해서 x * y로 변환하고 싶은가
4. stride: 필터를 적용하는 스트라이드의 크기(기본값은 1) => kernel이 한번에 얼마나 움직이는가?
5. padding: 입력의 각 측면에 추가할 패딩의 크기(기본값은 0) => convolution 진행할 때 가장자리에 data를 추가하는가 마는가
6. bias: 편향을 사용할지 여부를 결정하는 bool(기본값은 True)

$$ Output.size = \frac{input.size - kernel.size + padding}{stride}$$



```python
conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=0, bias=True)

# 예시를 위해 배치 크기가 1, 입력 채널이 3, 이미지 크기가 32x32인 텐서 생성
input_tensor = torch.randn(1, 3, 32, 32)

# 생성한 컨볼루션 레이어에 입력 데이터를 전달하여 출력 데이터를 얻음
output_tensor = conv_layer(input_tensor)


```

### Maxpooling Layer 설정
- pooling은 Convolution Layer에서 data 차원 압축을 위해 사용됨.
- 따라서 in_channel, out_channel 같은 channel 설정도 필요없고, 기존 data를 어떻게 압축하냐에 대한 정보만 존재.


```python
# kernel_size: 풀링을 적용할 윈도우의 크기.
# stride: 윈도우를 이동시키는 스트라이드의 크기. 기본값은 kernel_size와 동일
# padding: 입력의 각 측면에 추가할 패딩의 크기. 기본값은 0

max_pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)

# 예시를 위해 배치 크기가 1, 채널 수가 16, 이미지 크기가 32x32인 텐서 생성
input_tensor = torch.randn(1, 16, 32, 32)

# 생성한 MaxPool2d 레이어에 입력 데이터를 전달하여 출력 데이터를 얻음
output_tensor = max_pool(input_tensor)

print('입력 층:', input_tensor.size())
print('출력 층:', output_tensor.size())
```

## 3. Activation Function
- Neural Network에 비선형성을 넣어주기 위해서 도입하는 함수
    - 비선형성을 넣어주지 않으면 2개의 Layer가 1개로 압축되는 상황 발생
- Sigmoid, tanh, ReLU 등 다양한 Activation 함수 존재
- 제일 효율 잘 나오는걸로 사용

1. Sigmoid = 0과 1 사이로 압축, Classification에 주로 사용
2. Tanh = -1 ~ 1 사이로 압축, 정규화에 주로 사용
3. ReLU = 음수를 0으로 치환, 모델의 학습 속도 개선 및 gradient 소실 해결
4. LeakyReLu = 음수를 매우 작은 양수로 치환, ReLU + graident = 0으로 뉴런이 죽는 문제 해결
5. Softmax = MultiClassification에 사용
6. LogSoftmax = Softmax에 Log를 취한 값. 엔트로피 계산에 주로 사용



```python
linear_layer = nn.Linear(in_features=10, out_features=1)

input_tensor = torch.randn(1, 10)
linear_output = linear_layer(input_tensor)

# 시그모이드 활성화 함수 적용
sigmoid_output = torch.sigmoid(linear_output)

tanh_output = torch.tanh(linear_output)

relu_layer = nn.ReLU()
relu_output = relu_layer(linear_output)

leaky_relu = nn.LeakyReLU(negative_slope=0.01)
leaky_relu_output = leaky_relu(linear_output)

# Softmax 활성화 레이어 정의 (dim 파라미터는 softmax를 적용할 차원)
softmax = nn.Softmax(dim=1)
softmax_output = softmax(linear_output)
```

## 4. DropOut
- 학습 과정에서 일부로 몇몇 뉴런을 빼고 진행하기
- 모델이 특정 뉴런에 의해 과도하게 결정되는 걸 방지하기 위해 진행
- 대신 실제로 모델 평가할 때는 Dropout 진행 X / 모델을 학습할 때만 진행


```python
# 드랍아웃 객체 생성, 드랍아웃 비율을 50%로 설정
dropout = nn.Dropout(p=0.5)

# 입력 데이터 텐서 생성
input_data = torch.randn(1, 10)  # 1x10 크기의 랜덤 텐서

# 드랍아웃 적용
output = dropout(input_data)

print("Input data:\n", input_data)
print("\nAfter applying dropout:\n", output)
```

    Input data:
     tensor([[-0.0247,  0.0835,  0.8145,  0.9362,  0.7131,  0.1240, -1.8173, -0.2020,
              0.7798,  0.5650]])
    
    After applying dropout:
     tensor([[-0.0000,  0.1669,  0.0000,  0.0000,  1.4261,  0.2480, -3.6346, -0.0000,
              0.0000,  1.1300]])
    

## 5. 모델 만들기(Sequential)
- 지금까지 배운 Layer 만들기, Activation 함수, Dropout을 모두 결합해서 1개의 모델을 생성
- Sequential을 이용해 다 담으면 됨.

### 모델 만들기 예시
- 첫 번째 은닉층: 784개의 입력 특성을 받아 128개의 뉴런으로 전달합니다. 활성화 함수로는 ReLU를 사용합니다.
- 드랍아웃 적용: 첫 번째 은닉층 후에 드랍아웃을 적용합니다. 드랍아웃 확률은 20%입니다.
- 두 번째 은닉층: 128개의 뉴런을 받아 64개의 뉴런으로 전달합니다. 활성화 함수로는 ReLU를 사용합니다.
- 출력층: 64개의 뉴런을 받아 10개의 출력을 생성합니다(0부터 9까지의 숫자를 분류). 분류 문제이므로, 활성화 함수로는 LogSoftmax를 사용합니다. - (10개의 노드를 만들고, LogSoftmax의 dim값은 1로 적용해 주세요.)


```python
# Sequential 모델 정의
model = nn.Sequential(
    ### 여기에 모델 상세 내용을 작성하세요.
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128,64),
    nn.ReLU(),
    nn.Linear(64,10),
    nn.LogSoftmax(dim = 1)
)

# 모델 구조 출력
print(model)
```

    Sequential(
      (0): Linear(in_features=784, out_features=128, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.2, inplace=False)
      (3): Linear(in_features=128, out_features=64, bias=True)
      (4): ReLU()
      (5): Linear(in_features=64, out_features=10, bias=True)
      (6): LogSoftmax(dim=1)
    )
    

### 모델 만들기를 Class로 구현 가능. Class로 구현함으로써 범용성 증가


```python
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        # 첫 번째 은닉층: 784개의 입력 특성을 받아 128개의 뉴런으로 전달
        self.fc1 = nn.Linear(784, 128)
        self.relu1 = nn.ReLU()  # 첫 번째 은닉층의 활성화 함수로 ReLU 정의

        # 두 번째 은닉층: 128개의 뉴런을 받아 64개의 뉴런으로 전달
        self.fc2 = nn.Linear(128,64)
        self.relu2 = nn.ReLU()  # 두 번째 은닉층의 활성화 함수로 ReLU 정의

        # 출력층: 64개의 뉴런을 받아 10개의 출력을 생성
        self.fc3 = nn.Linear(64,10)

        # 드랍아웃 적용: 확률은 20%
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # 첫 번째 은닉층을 통과한 후 ReLU 활성화 함수 적용
        x = self.relu1(self.fc1(x))
        
        # 드랍아웃 적용
        x = self.dropout(x)
        
        # 두 번째 은닉층을 통과한 후 ReLU 활성화 함수 적용
        x = self.relu2(self.fc2(x))
        
        # 출력층을 통과
        x = self.fc3(x)
        
        # 최종 출력을 위해 Softmax 적용
        return nn.Softmax(dim=1)(x)

model = CustomModel()
model
```




    CustomModel(
      (fc1): Linear(in_features=784, out_features=128, bias=True)
      (relu1): ReLU()
      (fc2): Linear(in_features=128, out_features=64, bias=True)
      (relu2): ReLU()
      (fc3): Linear(in_features=64, out_features=10, bias=True)
      (dropout): Dropout(p=0.2, inplace=False)
    )


