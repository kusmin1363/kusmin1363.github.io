---
layout: single
title:  "DL-11. Seq2Seq"
---

# Seq2Seq

## 1. Seq2Seq란?
- 입력 시퀀스를 다른 형태의 출력 시퀀스로 변환하기 위해 고안된 모델 아키텍처입니다. 주로 기계 번역, 문서 요약, 챗봇 등 다양한 NLP 태스크에 사용됩니다. 
- Encoder, Decoder 2가지로 구성

![png](/assets/images/DL-11-image.png)

1. Encoder (인코더):  
![png](/assets/images/DL-11-image-2.png)  
입력 시퀀스를 받아서 내부의 은닉 상태(hidden state)들, 또는 최종 은닉 상태를 생성합니다.  
일반적으로 RNN, LSTM, GRU 등이 사용되고, 입력 시퀀스의 정보를 컨텍스트 벡터(고정된 길이의 벡터)로 압축합니다.  
처리 과정) 입력 시퀀스 -> 토큰 -> 임베딩 벡터(시퀀스) -> LSTM, GRU를 이용해 각 토큰 시점에서의 은닉 상태 생성  
-> 최종 은닉 상태 = 컨텍스트 벡터

2. Decoder (디코더):  
![png](/assets/images/DL-11-image-3.png)  
인코더가 생성한 컨텍스트 벡터를 바탕으로 출력 시퀀스를 생성합니다.  
디코더 역시 RNN 계열을 사용하며, 매 시점마다 이전 출력(또는 시작 토큰)과 은닉 상태를 활용해 다음 토큰을 예측합니다.  
처리 과정)  
컨텍스트 벡터를 디코더의 초기 은닉 상태로 지정  
-> 디코딩 시작 토큰을 받아 임베딩 벡터로 변환  
-> 임베딩 벡터와 초기 은닉 상태를 바탕으로 첫번째 토큰 생성  
-> 생성된 토큰 => 다음 입력으로 사용  
-> 토큰 => 임베딩 벡터   
-> 임베딩 벡터와 두번째 은닉 상태를 바탕으로 두번째 토큰 생성  
-> (반복) -> 최종 시퀀스 생성

## 2. Seq2Seq 구현

### 인코더 구현


```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)  # 임베딩
        self.dropout = nn.Dropout(dropout)  # 드롭아웃
        self.gru = nn.GRU(input_size=emb_dim, # GRU
             hidden_size=hidden_dim,
             num_layers=n_layers,
             batch_first=True,
             dropout=dropout) 
        
    #src: 인코더에 입력되는 문장
    def forward(self, src):
        embedded = self.dropout(self.embedding(src))  # 임베딩 층, 드롭아웃 적용
        outputs, hidden = self.gru(embedded)  # GRU 층

        return outputs, hidden  # 인코더 출력
```


```python
# 하이퍼파라미터 정의
INPUT_DIM = 10000 # 단어사전의 단어 수
EMB_DIM = 3 # 임베딩 차원
HID_DIM = 5 # GRU의 히든 레이어의 차원
N_LAYERS = 2 # GRU 층의 수
DROPOUT = 0.5

# 인코더 모델 객체 생성
encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)

# 테스트용 텐서
input_seq = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long)

# Encode the input sequence
encoder_output, hidden = encoder(input_seq)

print("Encoder input shape:", input_seq.shape) # (단어 개수, 임베딩 차원 수)
print(hidden.shape) # ()
print(hidden)
```

    Encoder input shape: torch.Size([1, 5])
    torch.Size([2, 1, 5])
    tensor([[[ 0.0685, -0.3433, -0.1519, -0.3663,  0.0641]],
    
            [[-0.0024, -0.1459,  0.1795, -0.4737, -0.3853]]],
           grad_fn=<StackBackward0>)
    

### 디코더 구현
- 개념도대로 생각하면 안되고, 코드는 개념도 상황을 한번의 코드로 진행해버림


```python
import torch.nn.functional as F

class Decoder(nn.Module):
    '''
        output_dim: 출력 단어의 개수 (타겟 어휘 크기)
        emb_dim: 임베딩 차원
        hidden_dim: GRU의 h의 차원
        n_layers: GRU의 층의 수
        dropout: 드롭아웃 비율
        '''
    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim # 단어사전 단어 개수
        self.embedding = nn.Embedding(output_dim, emb_dim) # 임베딩
        self.gru = nn.GRU(input_size = emb_dim,
                         hidden_size = hidden_dim,
                         num_layers = n_layers,
                         batch_first = True,
                         dropout = dropout) # GRU
        self.fc_out = nn.Linear(hidden_dim, output_dim)# 완전연결층
        self.dropout = nn.Dropout(dropout) # 드랍아웃

    def forward(self, trg, hidden):
        trg = trg.unsqueeze(1)  # 입력 차원 증가: [batch_size, 1]
        embedded = self.dropout(self.embedding(trg))  # 드롭아웃 적용 임베딩
        output, hidden = self.gru(embedded, hidden)  # GRU 실행
        prediction = self.fc_out(output.squeeze(1))  # 선형 레이어를 통한 로짓 계산
        prediction = F.softmax(prediction, dim= 1)  # 소프트맥스 적용

        return prediction, hidden
```


```python
INPUT_DIM = 10000
OUTPUT_DIM = 10000
EMB_DIM = 3
HID_DIM = 5
N_LAYERS = 2
DROPOUT = 0.5

# 테스트용 텐서
input_seq = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long)
current_token = torch.tensor([101], dtype=torch.long)
print('input_seq', input_seq.shape) # 시퀀수의 수, 단어의 수
print('current_token', current_token.shape) # 단어의 수

encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)

# Decode the encoded output
_, context_vector = encoder(input_seq)
prediction, hidden = decoder(current_token, context_vector)

print(prediction.shape) # 단어 개수, 단어사전의 전체 단어 수
print(prediction)

```

    input_seq torch.Size([1, 5])
    current_token torch.Size([1])
    torch.Size([1, 10000])
    tensor([[8.0071e-05, 1.1581e-04, 6.6030e-05,  ..., 7.8667e-05, 1.1616e-04,
             1.5705e-04]], grad_fn=<SoftmaxBackward0>)
    

### Seq2Seq 모델 실습


```python
import random

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, trg_vocab_size, tokens):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.sos_token = tokens['sos_token'] #시작 토큰 추가
        self.eos_token = tokens['eos_token'] #종료 토큰 추가
        self.trg_vocab_size = trg_vocab_size #사전의 크기
    # teacher_force = 이전 토큰의 출력값을 다음 입력으로 쓰기 때문에 한번 틀리면 계속 틀리게 되어있음
    # 그걸 방지하기 위해서 토큰 출력값을 만들었으면 맞는지 틀렸는지 확인하는 것 = teacher_force
    def forward(self, src, trg=None, teacher_force_ratio=1.0, max_len=50):
        _, hidden = self.encoder(src) # 인코더 실행
        # 디코더 초기 입력 설정(배치마다 시작 토큰을 만들어줌)
        current_token = torch.tensor([self.sos_token] * src.size(0)).to(self.device)

        batch_size = src.shape[0]
        outputs = torch.zeros(batch_size, max_len, self.trg_vocab_size).to(self.device) # 최종 출력값 초기화 [batch_size, max_len, vocab_size]
        all_tokens = torch.zeros(batch_size, max_len).to(self.device)

        for t in range(1, max_len):
            output, hidden = self.decoder(current_token, hidden) # 디코더 실행 (단어 1개 예측)
            outputs[:, t, :] = output # 예측한 단어 출력에 반영
            top1 = output.argmax(1)
            all_tokens[:, t] = top1

            # teacher_force 부분
            # teacher_force 확률에 기반해서 진행할지 말지 결정. 진행되면 정답을 다음 입력으로 쓰고, 그렇지 않으면 예측값을 다음 입력으로 사요
            if teacher_force_ratio > 0 and trg is not None and t < trg.size(1): 
                current_token = trg[:, t] if random.random() < teacher_force_ratio else top1
            else:
                current_token = top1

            # 모든 토큰이 종료 토큰이면 forward 종료
            if (current_token == self.eos_token).all(): 
                break

        actual_lengths = (all_tokens != self.eos_token).sum(dim=1)

        return outputs, actual_lengths # 최종 출력
```


```python
INPUT_DIM = 10000
OUTPUT_DIM = 10000
EMB_DIM = 3
HID_DIM = 5
N_LAYERS = 2
DROPOUT = 0.5
device = 'cpu'
tokens = {'sos_token': 0, 'eos_token': 1}

src = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device).to(device)
trg = torch.tensor([[101, 102, 103, 104, 105]], dtype=torch.long).to(device)

encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
model = Seq2Seq(encoder, decoder, device, trg_vocab_size=OUTPUT_DIM, tokens=tokens).to(device)

prediction = model(src, trg)

print(prediction[0].shape) #[batch_size, trg_len, output_dim]
print(prediction[0])
```

    torch.Size([1, 50, 10000])
    tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,
              0.0000e+00, 0.0000e+00],
             [7.8123e-05, 1.3630e-04, 8.9551e-05,  ..., 1.0987e-04,
              1.2635e-04, 8.2880e-05],
             [7.3457e-05, 1.5114e-04, 1.0835e-04,  ..., 1.3678e-04,
              1.3829e-04, 7.8186e-05],
             ...,
             [6.9314e-05, 1.4881e-04, 1.0961e-04,  ..., 1.3773e-04,
              1.3496e-04, 7.3195e-05],
             [7.7421e-05, 1.1588e-04, 7.5051e-05,  ..., 8.2782e-05,
              1.1012e-04, 7.9502e-05],
             [7.4105e-05, 1.1620e-04, 8.0855e-05,  ..., 8.4174e-05,
              1.1730e-04, 7.5287e-05]]], grad_fn=<CopySlices>)
    


