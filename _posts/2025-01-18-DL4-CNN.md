---
layout: single
title:  "DL-4. CNN"
---

# 딥러닝 시작하기 - CNN

## 1. 이미지 데이터란?
- 이미지는 픽셀, 해상도, 채널, 차원이라는 4가지 구성요소가 존재

픽셀 : Picture Element의 줄임말로, 이미지를 구성하는 미세한 점들을 의미  
- 흑백 이미지는 0~255의 값(빛의 양)을 가지며, 0 = 검은색, 255 = 흰 색이다.
- 컬러 이미지는 흑백 이미지와 비슷하게 R, G, B의 각각의 값이 0 ~ 255 사이의 값을 가지며, 255이면 Red, Green, Blue의 색을 가진다.  

해상도 : 이미지의 크기를 의미하며, 이미지의 너비와 높이를 픽셀 단위로 표현한다.  
- 해상도가 높아질 수록 디테일한 색 표현이 가능해지지만, 그에 따른 처리 시간 증가, 파일 크기 증가도 따라온다.

채널 : 몇 가지의 정보를 사용하는가에 대한 내용
- 흑백 이미지는 색상 정보라기 보다는 밝기 정보로 흑 ~ 백의 색상을 표현한다.
- 컬러 이미지는 각 색상(R,G,B)에 대한 정보를 통해서 표현함으로 3가지 정보를 사용하고, 이를 합쳐 다양한 색상을 구현하는 것이다.  

차원 : 해상도 * 채널을 의미. H * W * C (픽셀 높이 * 픽셀 너비 * 채널)


```python
from PIL import Image
import matplotlib.pyplot as plt

image_ex_path = 'C:\스터디\DACON\DL\example\example.png'
image_ex = Image.open(image_ex_path)
```

    <>:4: SyntaxWarning: invalid escape sequence '\D'
    <>:4: SyntaxWarning: invalid escape sequence '\D'
    C:\Users\user\AppData\Local\Temp\ipykernel_3352\3709939873.py:4: SyntaxWarning: invalid escape sequence '\D'
      image_ex_path = 'C:\스터디\DACON\DL\example\example.png'
    


```python
import numpy as np

image_ex = np.array(image_ex)

plt.imshow(image_ex, cmap='gray')
plt.axis('off') 
plt.show()
```

![png](/assets/images/2025-01-18-DL4-CNN_5_0.png)
    



```python
print(f"이미지 해상도: {image_ex.shape}")
print(f"픽셀 값의 범위: {image_ex.min()} to {image_ex.max()}")
```

    이미지 해상도: (512, 512, 4)
    픽셀 값의 범위: 0 to 255
    


```python
# 컬러 이미지의 RGB 채널 분리
image_color = np.array(image_ex)
r_channel = image_color[:, :, 0]
g_channel = image_color[:, :, 1]
b_channel = image_color[:, :, 2]

# 원본 컬러 이미지, 흑백 이미지, 그리고 RGB 채널 이미지를 시각화
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# 원본 컬러 이미지
axes[0, 0].imshow(image_color)
axes[0, 0].set_title("Original Color Image")
axes[0, 0].axis('off')

# R 채널
axes[0, 1].imshow(r_channel, cmap='Reds')
axes[0, 1].set_title("Red Channel")
axes[0, 1].axis('off')
# G 채널
axes[1, 0].imshow(g_channel, cmap='Greens')
axes[1, 0].set_title("Green Channel")
axes[1, 0].axis('off')
# B 채널
user_check = axes[1, 1].imshow(b_channel, cmap='Blues')
axes[1, 1].set_title("Blues Channel")
axes[1, 1].axis('off')

plt.show()
```


    
![png](/assets/images/2025-01-18-DL4-CNN_7_0.png)
    


## 2. CNN이란?
- 이미지 데이터를 단순히 보여주는 것은 이제 가능해졌다. 대신 어떻게 해야 이미지에 담긴 사물, 사람 등을 컴퓨터에 인식시킬 수 있을까?  

기존의 DNN 구조에서는 데이터에 따른 가중치 부여에 대한 판단을 진행했다. 대신 이미지 데이터는 다른 종류의 데이터보다 데이터의 크기도 훨씬 크며, 데이터간의 유기적인 연결이 중요하다(픽셀을 1개씩 보는게 아닌, 픽셀이 전체적으로 합쳐진 그림을 보기 때문). 따라서 CNN을 통해 각 픽셀 주변의 관계를 유지한 채로 이미지를 분석한다.

- CNN은 Feature Extraction, Classification의 2가지 과정으로 이미지를 분석한다.  
1. Feature Extraction : 필터를 통해 이미지에서 핵심적인 정보를 담고 있는 부분을 걸러낸다.
2. Classification : 핵심 특징을 통해 새롭게 들어온 데이터를 통해 분류해낸다.

### 2-1. Feature Extraction
- Input Data가 Filter와 합쳐져서 Feature Map으로 변질된다.
- Input Data를 Filter가 쓱 훑어서 각 영역에서 연산을 진행하고, 그 연산값만을 모은 Map을 생성한다. 이 과정을 통해 기존 Input Data보다 크기가 줄어들지만, 핵심적인 정보만 함축하게 된다.
- 컬러 이미지 같이 Channel이 여러 개인 경우에는, Filter의 크기가 Channel의 크기와 같도록 설정을 해준다. 예를 들어 1920 * 1080 * 3이라는 이미지 데이터를 슬라이딩 할 때는 5*5*3처럼 Channel과 Filter의 깊이가 같도록 설정해준다. 깊이가 같도록 설정해주면 R 층에서의 중요 정보, G에서의 중요 정보, B에서의 중요 정보를 각각 따로 모아 합쳐주면서, 이미지의 중요 정보를 다시 컬러 이미지로 변환하게 된다.

## 3. 실습 1 흑백 이미지


```python
import numpy as np
import matplotlib.pyplot as plt

# 이미지 (3x3 행렬)
image_grey = np.array([
    [1, 1, 0],
    [0, 1, 0],
    [1, 0, 1]
])

# 필터 (2x2 행렬)
filter_grey = np.array([
        [1,0],
        [0,1]
])

print("흑백 이미지:\n",image_grey)
print("필터:\n",filter_grey)
```

    흑백 이미지:
     [[1 1 0]
     [0 1 0]
     [1 0 1]]
    필터:
     [[1 0]
     [0 1]]
    


```python
# 특징 맵 생성 함수
def apply_convolution(image_grey, filter_grey):

    size = filter_grey.shape[0]
    height, width = image_grey.shape
    result = np.array([
        [height - size + 1, width - size + 1],
        [height - size + 1, width - size + 1]
    ])

    for i in range(result.shape[0]):
        for j in range(result.shape[1]):

            # 이미지의 부분 행렬과 필터의 합성곱 수행
            result[i, j] = np.sum(image_grey[i:i+size, j:j+size] * filter_grey)
    return result

# 컨볼루션 적용
feature_map = apply_convolution(image_grey, filter_grey)

# 결과 출력
print('흑백 이미지:\n', image_grey)
print('필터:\n', filter_grey)
print('컨볼루션을 적용한 피처맵:\n', feature_map)
```

    흑백 이미지:
     [[1 1 0]
     [0 1 0]
     [1 0 1]]
    필터:
     [[1 0]
     [0 1]]
    컨볼루션을 적용한 피처맵:
     [[2 1]
     [0 2]]
    


```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.title('Original Image_grey')
plt.imshow(image_grey, cmap='gray', interpolation='nearest')  # 이미지 시각화

plt.subplot(1, 2, 2)
plt.title('Feature Map')
plt.imshow(feature_map, cmap='gray', interpolation='nearest')  # 특징 맵 시각화

plt.show()
```


    
![png](/assets/images/2025-01-18-DL4-CNN_13_0.png)
    


## 4. 실습 2 컬러 이미지


```python
# 빨간색 채널
red_channel = np.array([
    [1, 2, 0, 2, 1],
    [0, 1, 1, 0, 0],
    [1, 0, 2, 0, 1],
    [0, 1, 1, 2, 0],
    [1, 0, 1, 0, 0]
])

# 녹색 채널
green_channel = np.array([
    [0, 1, 1, 0, 1],
    [0, 2, 1, 1, 0],
    [0, 0, 2, 0, 1],
    [0, 0, 1, 1, 0],
    [1, 0, 2, 2, 0]
])

# 파란색 채널
blue_channel = np.array([
    [1, 0, 2, 0, 1],
    [0, 0, 0, 1, 1],
    [1, 0, 2, 1, 2],
    [1, 0, 1, 0, 0],
    [0, 0, 1, 2, 0]
])

# 3차원 입력 배열
image_color = np.stack((red_channel, green_channel, blue_channel), axis=-1)

#print("컬러 이미지 값:\n",image_color)
print("컬러 이미지의 형태:\n",image_color.shape)
```

    컬러 이미지의 형태:
     (5, 5, 3)
    


```python
# 필터의 각 채널
filter_red_channel = np.array([[1, 0], [0, 1]])
filter_green_channel = np.array([[1, 2], [0, 1]])
filter_blue_channel = np.array([[2, 0], [0, 0]])

# 3차원 필터 배열
filter_color = np.stack((filter_red_channel, filter_green_channel, filter_blue_channel), axis = -1)

#print("필터 값:\n",filter_color) 
print("필터의 형태:\n",filter_color.shape)
```

    필터의 형태:
     (2, 2, 3)
    


```python
# 컨볼루션 수행 함수
def apply_3d_convolution(image_color, filter_color):
    filter_size_x, filter_size_y, filter_depth = filter_color.shape
    image_color_height, image_color_width, image_color_depth = image_color.shape

    # 출력 이미지의 크기 결정 (스트라이드는 1로 가정)
    output_height = image_color_height - filter_size_x + 1
    output_width = image_color_width - filter_size_y + 1
    output = np.zeros((output_height, output_width))

    # 컨볼루션 연산 수행
    for x in range(output_height):
        for y in range(output_width):
            # 이미지와 필터의 각 지점에서의 원소별 곱셈 합
            output[x, y] = np.sum(image_color[x:x+filter_size_x, y:y+filter_size_y, :] * filter_color)

    return output


# 컨볼루션 적용
feature_map = apply_3d_convolution(image_color, filter_color)

# 결과 출력
print("Feature Map after Convolution:\n", feature_map)
```

    Feature Map after Convolution:
     [[ 8.  7.  6.  4.]
     [ 4.  9.  4.  5.]
     [ 4.  6. 11.  4.]
     [ 2.  6.  8.  3.]]
    

## 5. 실습 3. Stride Padding 실습


```python
import torch
import torch.nn as nn
input_tensor = torch.tensor(
    [[[[ 20,  18,  29, 168,  75, 109,  77, 172],
       [122,  87, 154, 149, 159, 179, 253,  18],
       [ 79,  73, 174,  78, 189,  30,  49,  76],
       [122, 174,  38, 249, 208, 182, 113,  81],
       [ 17, 128,  13,  86, 149, 222, 135,  17],
       [236,  85,  99,  57,  13,  55, 223, 117],
       [177,  32, 168, 150, 160, 199, 196, 187],
       [155, 124, 191,  56, 207,  44, 203,  83]]]],
    dtype=torch.float32  
)

print("입력 데이터\n",input_tensor)
print("입력 데이터의 형태\n", input_tensor.shape)
```

    입력 데이터
     tensor([[[[ 20.,  18.,  29., 168.,  75., 109.,  77., 172.],
              [122.,  87., 154., 149., 159., 179., 253.,  18.],
              [ 79.,  73., 174.,  78., 189.,  30.,  49.,  76.],
              [122., 174.,  38., 249., 208., 182., 113.,  81.],
              [ 17., 128.,  13.,  86., 149., 222., 135.,  17.],
              [236.,  85.,  99.,  57.,  13.,  55., 223., 117.],
              [177.,  32., 168., 150., 160., 199., 196., 187.],
              [155., 124., 191.,  56., 207.,  44., 203.,  83.]]]])
    입력 데이터의 형태
     torch.Size([1, 1, 8, 8])
    


```python
# stride = 1일때의 Convolution
conv_layer_stride_one = nn.Conv2d(1, 1, kernel_size=3, stride=1)
output_tensor_stride = conv_layer_stride_one(input_tensor)

print('Output shape with stride:', output_tensor_stride.shape)

# stride = 2일때의 Convolution
conv_layer_stride_two = nn.Conv2d(1, 1, kernel_size=3, stride=2)
output_tensor_stride = conv_layer_stride_two(input_tensor)

print('Output shape with stride:', output_tensor_stride.shape)

# stride = 2, padding = 1일때의 Convolution, 참고로 padding이면 0값을 넣어서 부풀림.
conv_layer_stride_padding = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1)
output_tensor_stride_padding = conv_layer_stride_padding(input_tensor)

print('Output shape with stride and padding:', output_tensor_stride_padding.shape)

# stride = 2, padding = 2일때의 Convolution
conv_layer_stride_padding = nn.Conv2d(1,1,kernel_size=3,stride=2,padding=2)
output_tensor_stride_padding = conv_layer_stride_padding(input_tensor)

print('Output shape with stride and padding:', output_tensor_stride_padding.shape)
```

    Output shape with stride: torch.Size([1, 1, 6, 6])
    Output shape with stride: torch.Size([1, 1, 3, 3])
    Output shape with stride and padding: torch.Size([1, 1, 4, 4])
    Output shape with stride and padding: torch.Size([1, 1, 5, 5])
    

## 6. 실습 4. Pooling 실습


```python
feature_map = torch.tensor([[[
    [1, 2, 3, 4, 5, 6],
    [7, 8, 9, 10, 11, 12],
    [13, 14, 15, 16, 17, 18],
    [19, 20, 21, 22, 23, 24],
    [25, 26, 27, 28, 29, 30],
    [31, 32, 33, 34, 35, 36]
]]], dtype=torch.float32)

## feature_map.shape은 (batch_size, channel, height, width)로 나타남. 지금은 (1,1,6,6) 상태
```


```python
max_pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)
pooled_feature_map = max_pool_layer(feature_map)

print(feature_map[0, 0])  # 첫 번째 채널의 피처맵 출력 / 첫번째 batch의 첫번째 channel 전체를 의미
print('MaxPooled Feature Map:')
print(pooled_feature_map[0, 0])  # 풀링된 피처맵 출력 / 마찬가지

Avg_pool_layer = nn.AvgPool2d(kernel_size=2, stride=2)
pooled_feature_map = Avg_pool_layer(feature_map)


print('AvgPooled Feature Map:')
print(pooled_feature_map[0, 0]) 
```

    tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],
            [ 7.,  8.,  9., 10., 11., 12.],
            [13., 14., 15., 16., 17., 18.],
            [19., 20., 21., 22., 23., 24.],
            [25., 26., 27., 28., 29., 30.],
            [31., 32., 33., 34., 35., 36.]])
    MaxPooled Feature Map:
    tensor([[ 8., 10., 12.],
            [20., 22., 24.],
            [32., 34., 36.]])
    AvgPooled Feature Map:
    tensor([[ 4.5000,  6.5000,  8.5000],
            [16.5000, 18.5000, 20.5000],
            [28.5000, 30.5000, 32.5000]])
    


```python
import numpy as np
import matplotlib.pyplot as plt

# 빨간색 채널
red_channel = np.array([
    [1, 2, 0, 2, 1],
    [0, 1, 1, 0, 0],
    [1, 0, 2, 0, 1],
    [0, 1, 1, 2, 0],
    [1, 0, 1, 0, 0]
])

# 녹색 채널
green_channel = np.array([
    [0, 1, 1, 0, 1],
    [0, 2, 1, 1, 0],
    [0, 0, 2, 0, 1],
    [0, 0, 1, 1, 0],
    [1, 0, 2, 2, 0]
])

# 파란색 채널
blue_channel = np.array([
    [1, 0, 2, 0, 1],
    [0, 0, 0, 1, 1],
    [1, 0, 2, 1, 2],
    [1, 0, 1, 0, 0],
    [0, 0, 1, 2, 0]
])

# 3차원 입력 배열
image_color = np.stack((red_channel, green_channel, blue_channel), axis=-1)

# 필터의 각 채널
filter_red_channel = np.array([[1, 0], [0, 1]])
filter_green_channel = np.array([[1, 2], [0, 1]])
filter_blue_channel = np.array([[2, 0], [0, 0]])

# 3차원 필터 배열
filter_color = np.stack((filter_red_channel, filter_green_channel, filter_blue_channel), axis = -1)
```

정리하면:

시작: (5, 5, 3) → 높이, 너비, 채널 순서
unsqueeze(0): (1, 5, 5, 3) → 배치 차원 추가
permute(0, 3, 1, 2): (1, 3, 5, 5) → PyTorch에서 요구하는 (배치, 채널, 높이, 너비) 순서
이 과정을 통해, 컬러 이미지 데이터를 PyTorch의 CNN에 입력하기 적합한 형태로 변환할 수 있습니다.


```python
input_tensor = torch.tensor(image_color, dtype=torch.float).unsqueeze(0).permute(0, 3, 1, 2)

filter_tensor = torch.tensor(filter_color, dtype=torch.float).unsqueeze(0)  
filter_tensor = filter_tensor.permute(0,3,1,2) 

conv_layer = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2)

with torch.no_grad():
    conv_layer.weight = nn.Parameter(filter_tensor)
    conv_layer.bias = nn.Parameter(torch.zeros(1)) 

output_tensor = conv_layer(input_tensor)

print('Output shape after convolution:', output_tensor.shape)
print('Output tensor after convolution:', output_tensor)
```

    Output shape after convolution: torch.Size([1, 1, 4, 4])
    Output tensor after convolution: tensor([[[[ 8.,  7.,  6.,  4.],
              [ 4.,  9.,  4.,  5.],
              [ 4.,  6., 11.,  4.],
              [ 2.,  6.,  8.,  3.]]]], grad_fn=<ConvolutionBackward0>)
    


```python
#Convolution이후 MaxPooling 진행
max_pool_layer = nn.MaxPool2d(2,2)
output_tensor_max_pool = max_pool_layer(output_tensor)

print('Output shape after max pooling:', output_tensor_max_pool.shape)
print('Output tensor after max pooling:', output_tensor_max_pool)

#Convolution 이후 AvgPooling 진행
avg_pool_layer = nn.AvgPool2d(2,2)
output_tensor_avg_pool = avg_pool_layer(output_tensor)
                                        
print('Output shape after average pooling:', output_tensor_avg_pool.shape)
print('Output tensor after average pooling:', output_tensor_avg_pool)
```

    Output shape after max pooling: torch.Size([1, 1, 2, 2])
    Output tensor after max pooling: tensor([[[[ 9.,  6.],
              [ 6., 11.]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)
    Output shape after average pooling: torch.Size([1, 1, 2, 2])
    Output tensor after average pooling: tensor([[[[7.0000, 4.7500],
              [4.5000, 6.5000]]]], grad_fn=<AvgPool2DBackward0>)
    

## 실습 5. Flatten


```python
import torch
import torch.nn as nn

x = torch.rand(1, 3, 5, 5)

flatten = nn.Flatten()
x_flattened = flatten(x)

print("Original shape:", x.shape)
print("x_flattened shape:", x_flattened.shape)

```

    Original shape: torch.Size([1, 3, 5, 5])
    x_flattened shape: torch.Size([1, 75])
    


```python
fc_layer = nn.Linear(in_features=75, out_features=3)  
activation = nn.ReLU()  

output = activation(fc_layer(x_flattened))  

print(output) 
```

    tensor([[0.0000, 0.1158, 0.1153]], grad_fn=<ReluBackward0>)
    


```python
# Flatten 거치고, Fully connected 를 3번 거친 후에, SOftmax 함수 적용
import torch.nn.functional as F

fc_layer1 = nn.Linear(in_features=75, out_features=40)  
fc_layer2 = nn.Linear(in_features=40, out_features=20)
fc_layer3 = nn.Linear(in_features=20, out_features=3)   
activation = nn.ReLU()  # 활성화 함수로 ReLU 사용

output1 = activation(fc_layer1(x_flattened)) 
output2 = activation(fc_layer2(output1))       
output3 = fc_layer3(output2)                   

probabilities = F.softmax(output3, dim=1)

print("Probabilities:", probabilities)
print("Sum of probabilities:", torch.sum(probabilities))
```

    Probabilities: tensor([[0.3241, 0.3321, 0.3438]], grad_fn=<SoftmaxBackward0>)
    Sum of probabilities: tensor(1., grad_fn=<SumBackward0>)
    


```python
flatten = nn.Flatten()
flattened = flatten(output_tensor_max_pool)  

fc1 = nn.Linear(in_features=4, out_features=50)  
fc2 = nn.Linear(in_features=50, out_features=3)  

relu = F.relu
softmax = F.softmax 

hidden_output = relu(fc1(flattened))  
output = softmax(fc2(hidden_output)) 

print("Flattened input:\n", flattened)
print("\nHidden layer output shape:\n", hidden_output.shape)
print("\nFinal output shape:\n", output.shape)
print("\nFinal output (probabilities):\n", output)
```

    Flattened input:
     tensor([[ 9.,  6.,  6., 11.]], grad_fn=<ViewBackward0>)
    
    Hidden layer output shape:
     torch.Size([1, 50])
    
    Final output shape:
     torch.Size([1, 3])
    
    Final output (probabilities):
     tensor([[0.6862, 0.1431, 0.1707]], grad_fn=<SoftmaxBackward0>)
    

    C:\Users\user\AppData\Local\Temp\ipykernel_3352\2324372656.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
      output = softmax(fc2(hidden_output))
    


```python

```
