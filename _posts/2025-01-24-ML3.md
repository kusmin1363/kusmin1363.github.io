---
layout: single
title:  "3일차. MLE"
---

## 목차
1. [Parametric Method](#1-parametric-method)
2. [MLE](#2-mle)
3. [MLE for Bernoulli Distribution](#3-mle-for-bernoulli-distribution)
4. [MLE for Categorical Distribution](#4-mle-for-categorical-distribution)
5. [MLE for Supervised Learning](#5-mle-for-supervised-learning)

### 1. Parametric Method
- 분포를 알고 있는 상황에서, 분포의 핵심 parameter를 모르고 있는 상황 자체를 의미한다.
- 예를 들어 Data x가 정규분포를 따를 때, 정규분포의 $\mu, \sigma$ 를 추정해야 하는 상황
- 기계학습을 공부할 때 Parametric Method, Semi-parametrice Method, Nonparametric Method 크게 3개로 나뉜다.

### 2. MLE
- Maxminum Likelihood Estimation의 줄임말로, 쉽게 말해 Likelihood 값이 제일 높은 걸로 추정하겠다는 의미다.
- $ D = {\{x^t\}}_{t=1}^N $ 라고 하고, 모든 x가 iid일 때, $\theta$ 에 대한 Likelihood
$ \Rightarrow P(D\mid\theta) = P(x^1, x^2, ...\;, x^N\mid\theta) = \prod _t P(x^t\mid\theta) $
- $P(D\mid\theta)$ 는 쉽게 말해 $\theta$ 값에 따라 D가 만들어 질 수 있는 가능성(확률)을 의미한다.

보통 Likelihood에서 log를 취한 Log likelihood를 자주 사용한다.
- $\log P(D\mid\theta) = \log \prod _t P(x^t\mid\theta) = \sum _t \log P(x^t\mid\theta)$

Maxmium Likelihood(MLE)
- $\underset{\theta}{\arg\max}P(D\mid\theta) = \underset{\theta}{\arg\max}\prod _t P(x^t\mid\theta) = \underset{\theta}{\arg\max}\sum _t \log P(x^t\mid\theta)$

#### 여러 Distribution에 직접 적용시켜보자

### 3. MLE for Bernoulli Distribution
- Bernoulli Distribution이란 확률($\theta$)에 따라 성공하면 1, 아니면 0의 값을 가지는 분포이다.
    - => $P(x\mid\theta) = \theta^x(1-\theta)^{1-x}$ 

MLE 적용 
* $\underset{\theta}{\arg\max}P(D\mid\theta) = \underset{\theta}{\arg\max}\prod _t P(x^t\mid\theta) = \underset{\theta}{\arg\max}\sum _t \log P(x^t\mid\theta)= \underset{\theta}{\arg\max}\underbrace{\sum{_t} [x^t\log\theta + (1-x^t)log(1-\theta)]}_L$  

* $\frac{\partial{L}}{\partial{\theta}} = \sum_t[x^t\frac{1}{\theta}-(1-x^t)\frac{1}{1-\theta}] = \sum_t[\frac{x^t-x^t\theta-\theta+x^t\theta}{\theta(1-\theta)}] = 0 $  
$\qquad \Rightarrow \sum_t[x^t-\theta] = 0$  
  

$\quad \therefore\,\theta = \frac{\sum_t x^t}{N}$


### 4. MLE for Categorical Distribution
- Categorical Distribution이란 Bernoulli와 비슷하지만, 확률에 따라 좀 더 다양한 값을 가진다.  
v = 특정 클래스에 해당하는 확률변수 값, $\theta$ = 클래스가 나올 수 있는 확률, i = 클래스 번호(1~K)  
$ P(x = v_i) = \theta_i \Rightarrow P(x\mid\theta) = \prod_i{\theta}_i^{1(x=v_i)}$ 

#### 4-1. 라그랑주 승수법  
- 함수 f(x)의 최소값을 찾아야 하되, 다른 조건(g(x) = 0)을 유지하면서 찾을 때 사용, 많이 사용하기 때문에 알고 있으면 유용하다.  
- 단순 f(x) 사용 대신, $L = f(x) - \lambda g(x)$ 로 설정하고 $ \nabla L = 0 $으로 계산한다

ex  
$ f(x_1,x_2) = x_1^2+ x_2^2 \quad$ & $\quad$조건 : $x_1 + x_2 = 1 \Rightarrow g(x_1,x_2) = x_1+x_2 -1 = 0$  
$ \nabla f(x_1,x_2) = \langle 2x_1, 2x_2 \rangle \quad \nabla g(x_1,x_2) = \langle1,1\rangle$  

L을 최소화하기 위해 $\nabla L = 0$을 이용  
1. $\frac{\partial{L}}{\partial{x_1}} = 2x_1 - \lambda = 0 \quad \therefore x_1 = \frac{\lambda}{2}$
2. $\frac{\partial{L}}{\partial{x_2}} = 2x_2 - \lambda = 0 \quad \therefore x_2 = \frac{\lambda}{2}$
3. $\frac{\partial{L}}{\partial{\lambda}} = x_1 + x_2 - 1 = 0 \quad \therefore \lambda = 1, x_1 = x_2 = \frac{1}{2}$  
결국 f(x)의 최소값 = $x_1^2+x_2^2 = 0.5^2 + 0.5^2 = 0.5$

#### 4-2. MLE for Categorical Distrubition
$ P(x = v_i) = \theta_i \Rightarrow P(x\mid\theta) = \prod_i{\theta}_i^{1(x=v_i)}$ 
MLE 적용 
* $\underset{\theta,\sum_i\theta_i = 1}{\arg\max}P(D\mid\theta) = \underset{\theta,\sum_i\theta_i = 1}{\arg\max}\prod _t P(x^t\mid\theta) = \underset{\theta,\sum_i\theta_i = 1}{\arg\max}\sum _t \log P(x^t\mid\theta)$  
$ = \underset{\theta}{\arg\max}\underbrace{[\sum_t\sum_i 1(x^t=v_i)\log\theta_i - \lambda(\sum_i\theta_i-1)]}_L$ (라그랑주 승수법 적용)  
*  $\frac{\partial{L}}{\partial{\theta_i}} = \sum_t 1(x^t = v_i)\frac{1}{\theta_i} - \lambda = 0 \Rightarrow \theta_i = \frac{\sum_t 1(x^t=v_i)}{\lambda}\quad \text{for\;all\;i} $ 
*  $\frac{\partial{L}}{\partial{\lambda}} = -\sum_i\theta_i + 1 = 0$  
$\Rightarrow \sum_i\theta_i = \frac{\sum_i\sum_t 1(x^t=v_i)}{\lambda} = 1 \Rightarrow \lambda = \sum_i\underbrace{\sum_t 1(x^t=v_i)}_1 = N$  
$\therefore \theta_i = \frac{\sum_t 1(x^t=v_i)}{N}, \quad \text{for\;all\;i}$  
직관적으로 이해하자면 전체 표본 개수(N) 중에서 클래스에 해당되는 만큼(분수 윗부분)을 확률로 생각할 수 있다는 뜻.

### 5. MLE for Supervised Learning
MLE for supervised learning of categorical distribution
- $D = \{x^t,r^t\}_{t=1}^N, x^t = \begin{bmatrix} x_1^t \\ \vdots \\ x_d^t \end{bmatrix}, r^t = \begin{bmatrix} r_1^t \\ \vdots \\ r_d^t \end{bmatrix}, r_k^t = \begin{cases} 1\quad\text{input이 class  k에 속할 때} \\ 0\quad\text{Otherwise}\end{cases} $
* $P(D\mid\theta) = \prod_t P(x^t,r^t\mid\theta) = \prod_t P(r^t\mid\theta)P(x^t\mid r^t,\theta) = \prod_t P(r^t\mid\theta)\prod_i P(x_i^t\mid r^t,\theta)$  
* $\log P(D\mid\theta) = \sum_t \log P(r^t\mid\theta) + \sum_t\sum_i \log P(x_i^t\mid r^t,\theta)$  
$P(r^t\mid\theta)$를 k개의 클래스에 대한 관점으로 확대 $\quad = \prod_k P(c_k \mid \dot{\theta})^{r_k^t}$   
$P(x_i^t\mid r^t,\theta)$도 k개의 클래스에 대한 관점으로 확대 $\quad = \prod_k P(x_i^t \mid c_k, \ddot{\theta})^{r_k^t}$ 이것들을 다시 log 안으로 대입  
$\underset{\theta}{\arg\max} P(D\mid\theta) = \begin{cases} \underset{\dot{\theta}, \mid\dot{\theta}\mid =1,}{\arg\max}\sum_t\log\prod_k P(c_k\mid\dot{\theta})^{r_k^t}\\  \underset{\ddot{\theta}_{k,i}, \mid\ddot{\theta}_{k,i}\mid =1,}{\arg\max}\sum_{t,r_k^t=1}\log\prod_k P(x_i\mid c_k, \ddot{\theta}_{k,i})^{r_k^t} \quad\text{for\;all\;k\;and\;i}\end{cases}$  

수식이 좀 어렵게 느껴질 수도 있다. 예를 들어서 정육면체 주사위를 굴려서 3 이상이면 클래스 1, 반대면 클래스 2라고 가정하자.  
$\dot{\theta}$는 주사위를 던져서 나오는 수(1~6)를 의미하고, $\ddot{\theta}$ 는 클래스 1, 2를 의미한다.

결론  
$P(c_k) \leftarrow \frac{\sum_t r_k^t}{N} \qquad\qquad\qquad\qquad\qquad\qquad\quad\quad\qquad ; \dot{\theta_k}\text{\;for\;all\;k}$  
$ P(x_i = v_j \mid c_k) \leftarrow \frac{\sum_{t,r_k^t = 1}1(x_i^t = v_j)}{\sum_t r_k^t} = \frac{\sum_{t}r_k^t = 1(x_i^t = v_j)}{\sum_t r_k^t} \qquad ; \ddot{\theta}_{k,i,j}\text{\;for\;all\;k,\;i,\;and\;j}$

따라서 결론은 아까 Categorical Distribution가 마찬가지로
특정 클래스의 확률 = 특정 클래스 등장 횟수($\sum_t r_k^t$) / 전체 데이터(N) 이고,  
특정 클래스에서 확률 변수의 값이 특정 값($v_j$)일 경우 = 확률 변수와 특정 값이 동일한 횟수(  $\sum_t r_k^t 1 (x_i^t = v_j) $  ) / 특정 클래스의 데이터 수($\sum_t r_k^t$)

#### MLE에 대해
- 어렵게 이해하기 보다는 우리가 상식적으로 행동하는 방면에서 이해하는게 낫다고 생각한다. 확률을 모를 때는, 현재 주어진 결과값이 제일 잘 나오는 확률로 상정하는게 이치에 맞다. MLE는 자주 사용하므로 개념적인 이해가 우선시되면 좋다.
