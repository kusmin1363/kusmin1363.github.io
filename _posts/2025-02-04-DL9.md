---
layout: single
title:  "DL-9. LSTM"
---

# 1. LSTM이란?


## 1. LSTM이 등장한 이유
- RNN 구조에 두 가지 한계점 발생. 
    1. tanh는 -1 ~ 1 사이의 값을 가지기 때문에, 이런 값들이 곱해지면서 점차 0으로 수렴함
    2. tanh가 -1, 1에 가까운 값을 가질 때면, 미분값이 0에 매우 가까워지면서 역전파가 진행될때 그래디언트가 소실되는 문제
- 종합하면, 너무 과거에 있었던 일은 현재 시점에 영향을 안 끼치게 되는 현상 발생 = 이를 **장기 의존성 문제**라고 함
- LSTM은 Long Short-Term Memory의 줄임말로, '셀 상태', '게이트 구조'를 도입해 장기 의존성 문제를 해결했다.  
![png](/assets/images/image-3.png)

1. Cell state = LSTM에서 "기억" 역할을 하는 핵심 요소. 시간에 따른 정보를 선형적으로 전달(곱셈과 덧셈으로만 진행)하기 때문에 기울기 소실 문제가 발생하지 않는다. 게이트들을 통해서 어떤 정보를 전달할지 정해진다.
2. Gate 구조 = Input Gate, Forget Gate, Output Gate 3가지 Gate를 지닌다.
    - Input Gate = 현재 입력으로 들어오는 새로운 정보 중에서 cell state에 얼마나 반영할 것인가 결정
    - Forget Gate = 이전 시점의 cell state에서 덜 중요한 정보를 얼마나 버릴것인가 결정(1 : 다 보유, 0 : 다 버림)
    - Output Gate = cell state의 정보를 얼마나 내보내서 Hidden State로 활용한것인가 결정  
    3가지 Gate 모두 각자만의 Weight와 Bias를 지닌다.
3. 수식으로 정리해보면 ($\sigma$ = sigmoid 함수, 0~1 사이의 값으로 정보 통과 비율을 설정)    
Forget Gate : $f_t = \sigma(W_f\cdot[h_{t-1}, x_t] + b_f)$  
Input Gate : $ i_t = \sigma(W_i \cdot [h_{t-1},x_t] + b_i)$   
            $\qquad\qquad\tilde{C_t} = tanh(W_c \cdot [h_{t-1}, x_t] + b_c)$ = 현재 입력을 바탕으로 생성된 Input 후보    
Update : $ C_t = f_t \ast C_{t-1} + i_t \ast \tilde{C_t}$ = 버릴건 좀 버리고, 후보를 조금 넣는다는 소리  
Output Gate : $o_t = \sigma(W_0[h_{t-1},x_t]+b_o)$  
$\qquad\qquad h_t = o_t \ast tanh(C_t)$  = C의 일부를 다시 h로 바꿔서 사용  
tanh = -1 ~ 1 사이의 값으로 cell-state가 너무 커지는 걸 방지

# 2. LSTM Cell state 및 Gate 작동시켜보기


```python
import torch
import torch.nn as nn

sequence_data = torch.tensor([
    [25.0, 70.0, 10.0, 0.0],  # 시점 t-5
    [22.0, 80.0, 5.0, 0.0],    # 시점 t-4
    [15.0, 80.0, 20.0, 5.0], # 시점 t-3
    [10.0, 90.0, 5.0, 20.0], # 시점 t-2
    [5.0, 85.0, 30.0, 10.0],  # 시점 t-1
    [18.0, 75.0, 10.0, 0.0]   # 시점 t
])
print(sequence_data[0])
print(sequence_data[0].shape)
print(sequence_data.shape)
```

    tensor([25., 70., 10.,  0.])
    torch.Size([4])
    torch.Size([6, 4])
    


```python
# 첫 번째 데이터 포인트 
input_data = sequence_data[0].unsqueeze(0)  # n_sequence = 1
input_size = input_data.shape[1]

# 초기 은닉 상태와 셀 상태 설정
hidden_size = 10  # 은닉 상태의 크기 설정

h_0 = torch.zeros(1, hidden_size)
c_0 = torch.zeros(1, hidden_size)

print('입력 데이터 형태:', input_data.shape)
print('입력 데이터 (특성) 크기:', input_size)

print('은닉 상태 형태:', h_0.shape)
print('셀 상태 형태:', c_0.shape)
```

    입력 데이터 형태: torch.Size([1, 4])
    입력 데이터 (특성) 크기: 4
    은닉 상태 형태: torch.Size([1, 10])
    셀 상태 형태: torch.Size([1, 10])
    

## Forget Gate


```python
Wx_f = nn.Parameter(torch.randn(input_size,hidden_size))
Wh_f = nn.Parameter(torch.randn(hidden_size, hidden_size))
b_f =nn.Parameter(torch.randn(hidden_size))
                               
# 망각  게이트 계산
f_t = torch.sigmoid(torch.matmul(input_data, Wx_f) + torch.matmul(h_0, Wh_f ) + b_f)

print('망각 게이트의 입력 가중치 행렬(Wx_f) 형태:', Wx_f.shape)
print('망각 게이트의 은닉상태 가중치 (Wh_f) 형태:', Wh_f.shape)
print('망각 게이트의 편향 벡터 (b_f) 형태:', b_f.shape)
print("f_t (망각 게이트 형태):", f_t.shape)
```

    망각 게이트의 입력 가중치 행렬(Wx_f) 형태: torch.Size([4, 10])
    망각 게이트의 은닉상태 가중치 (Wh_f) 형태: torch.Size([10, 10])
    망각 게이트의 편향 벡터 (b_f) 형태: torch.Size([10])
    f_t (망각 게이트 형태): torch.Size([1, 10])
    

## Input Gate


```python
Wx_i = nn.Parameter(torch.randn(input_size, hidden_size ))
Wh_i = nn.Parameter(torch.randn(hidden_size, hidden_size))
b_i = nn.Parameter(torch.randn(hidden_size))
                 
# 입력  게이트 계산
i_t = torch.sigmoid(torch.matmul(input_data,Wx_i ) + torch.matmul( h_0, Wh_i) + b_i)

print('입력 게이트의 입력 가중치 행렬(Wx_i) 형태:', Wx_i.shape)
print('입력 게이트의 은닉상태 가중치 행렬(Wh_i) 형태:', Wh_i.shape)
print('입력 게이트의 편향 벡터 (b_i) 형태:', b_i.shape)
print("i_t (입력 게이트 형태):", i_t.shape)  

# 후보 셀상태 혹은 셀게이트 계산
Wx_C = nn.Parameter(torch.randn(input_size, hidden_size))
Wh_C = nn.Parameter(torch.randn(hidden_size, hidden_size))
b_C = nn.Parameter(torch.randn(hidden_size))

# 후보 셀 상태 계산
C_hat_t = torch.tanh(torch.matmul(input_data, Wx_C) + torch.matmul(h_0, Wh_C) + b_C)

print('후보 셀상태의 입력 가중치 행렬(Wx_C) 형태:', Wx_C.shape)
print('후보 셀상태의 은닉상태 가중치 행렬(Wh_C) 형태:', Wh_C.shape)
print('후보 셀상태의 편향 벡터 (b_C) 형태:', b_C.shape)
print("C_hat_t (후보 셀상태 형태):", C_hat_t.shape)
```

    입력 게이트의 입력 가중치 행렬(Wx_i) 형태: torch.Size([4, 10])
    입력 게이트의 은닉상태 가중치 행렬(Wh_i) 형태: torch.Size([10, 10])
    입력 게이트의 편향 벡터 (b_i) 형태: torch.Size([10])
    i_t (입력 게이트 형태): torch.Size([1, 10])
    후보 셀상태의 입력 가중치 행렬(Wx_C) 형태: torch.Size([4, 10])
    후보 셀상태의 은닉상태 가중치 행렬(Wh_C) 형태: torch.Size([10, 10])
    후보 셀상태의 편향 벡터 (b_C) 형태: torch.Size([10])
    C_hat_t (후보 셀상태 형태): torch.Size([1, 10])
    

## Update


```python
# 셀 상태 계산
cx = f_t * c_0 + i_t * C_hat_t 
print(f"Cell state : {cx}")
print(f"Cell state shape: {cx.shape}")
```

    Cell state : tensor([[ 9.5432e-01,  6.5207e-38,  1.0000e+00, -1.0000e+00, -1.0000e+00,
              1.0000e+00, -2.7353e-26, -5.1508e-16, -1.4617e-14, -1.0000e+00]],
           grad_fn=<AddBackward0>)
    Cell state shape: torch.Size([1, 10])
    

## Output Gate 및 hidden-state 계산


```python
Wx_o = nn.Parameter(torch.randn(input_size, hidden_size ))
Wh_o = nn.Parameter(torch.randn(hidden_size, hidden_size))
b_o = nn.Parameter(torch.randn(hidden_size))
# 출력  게이트 계산
o_t = torch.sigmoid(torch.matmul(input_data, Wx_o ) + torch.matmul(h_0, Wh_o) + b_o)
# 은닉 상태 계산
hx = o_t * torch.tanh(cx)
print(f"Hidden state shape: {hx.shape}")
```

    Hidden state shape: torch.Size([1, 10])
    

# 3. LSTM Cell 만들기

## 1. 데이터 불러오기 + 전처리


```python
import numpy as np
import pandas as pd 
import random 
import datetime
train_data = pd.read_csv('C:\\스터디\\DACON\\시계열\\data\\train.csv')
features = ['humidity','rainfall','wspeed','temperature'] 
dict ={
    '평균습도' : 'humidity',
    '강수량' : 'rainfall',
    '평균풍속' : 'wspeed',
    '평균기온' : 'temperature',
    '일시' : 'day'
}
train_data = train_data[['일시', '평균습도', '강수량', '평균풍속', '평균기온']]
train_data = train_data.rename(columns = dict)
train_data['day'] = pd.to_datetime(train_data['day'])
train_data['rainfall'] = train_data['rainfall'].interpolate(method='linear', limit_direction='both')
df_weather = train_data[train_data['day'] >= '2021-01-01']
df_weather.reset_index(drop=True)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>day</th>
      <th>humidity</th>
      <th>rainfall</th>
      <th>wspeed</th>
      <th>temperature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-01-01</td>
      <td>64.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-4.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-01-02</td>
      <td>38.5</td>
      <td>0.0</td>
      <td>2.6</td>
      <td>-5.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-01-03</td>
      <td>45.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-5.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-01-04</td>
      <td>51.4</td>
      <td>0.0</td>
      <td>1.7</td>
      <td>-3.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-01-05</td>
      <td>52.8</td>
      <td>0.0</td>
      <td>2.9</td>
      <td>-5.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>725</th>
      <td>2022-12-27</td>
      <td>69.8</td>
      <td>0.6</td>
      <td>1.8</td>
      <td>-2.6</td>
    </tr>
    <tr>
      <th>726</th>
      <td>2022-12-28</td>
      <td>58.1</td>
      <td>0.1</td>
      <td>2.5</td>
      <td>-3.3</td>
    </tr>
    <tr>
      <th>727</th>
      <td>2022-12-29</td>
      <td>56.3</td>
      <td>0.0</td>
      <td>1.7</td>
      <td>-2.9</td>
    </tr>
    <tr>
      <th>728</th>
      <td>2022-12-30</td>
      <td>65.6</td>
      <td>0.0</td>
      <td>1.9</td>
      <td>-1.8</td>
    </tr>
    <tr>
      <th>729</th>
      <td>2022-12-31</td>
      <td>65.5</td>
      <td>0.0</td>
      <td>1.4</td>
      <td>-1.2</td>
    </tr>
  </tbody>
</table>
<p>730 rows × 5 columns</p>
</div>



## 2. 데이터를 시퀀스 데이터로 바꿔주기


```python
# 시퀀스 데이터 생성 함수
def build_sequence_dataset(df, seq_length):
    dataX = []
    dataY = []
    for i in range(0, len(df) - seq_length):
        _x = df.iloc[i:i + seq_length].values  # 시퀀스 데이터
        _y = df.iloc[i + seq_length]['temperature']  # 다음 포인트의 기온을 레이블로 사용
        dataX.append(_x)
        dataY.append(_y)
    return np.array(dataX), np.array(dataY)

# 시퀀스 길이 정의
seq_length = 6  # 과거 6일의 데이터를 기반으로 다음날의 기온을 예측
# 데이터셋 생성
sequence_dataX, sequence_dataY = build_sequence_dataset(df_weather[features], seq_length)

# 생성된 시퀀스 데이터의 첫 번째 요소를 출력하여 실습 예제 확인
print(f"sequence_dataX 형태 : {sequence_dataX.shape}")
print(f"sequence_dataX 갯수 : {len(sequence_dataX)}")

print("첫번째 sequenceX:", sequence_dataX[0])
print("첫번째 sequenceY:", sequence_dataY[0])

```

    sequence_dataX 형태 : (724, 6, 4)
    sequence_dataX 갯수 : 724
    첫번째 sequenceX: [[64.   0.   2.  -4.2]
     [38.5  0.   2.6 -5. ]
     [45.   0.   2.  -5.6]
     [51.4  0.   1.7 -3.5]
     [52.8  0.   2.9 -5.5]
     [54.6  2.3  2.4 -7.4]]
    첫번째 sequenceY: -14.5
    

## 3. LSTM 모델 생성


```python
import torch
import torch.nn as nn

num_features = len(features)
num_hidden = 10

# LSTM 모듈 초기화
model_lstm = nn.LSTM(input_size=num_features, hidden_size=num_hidden, num_layers=1)
print("모델 구조:")
print(model_lstm)

for name, param in model_lstm.named_parameters():
    print(f"{name}: {param.shape}")
```

    모델 구조:
    LSTM(4, 10)
    weight_ih_l0: torch.Size([40, 4])
    weight_hh_l0: torch.Size([40, 10])
    bias_ih_l0: torch.Size([40])
    bias_hh_l0: torch.Size([40])
    

## 4. 가중치 초기화
- nn.LSTM은 은닉 상태를 (레이어 개수, 배치 개수, 은닉 유닛 개수)로 지정


```python
h0 = torch.zeros(1, 1, num_hidden)
c0 = torch.zeros(1, 1, num_hidden)

print("Hidden state shape:", h0.shape)
print("Cell state shape:", c0.shape)
```

    Hidden state shape: torch.Size([1, 1, 10])
    Cell state shape: torch.Size([1, 1, 10])
    

## 5. 시퀀스 데이터 Tensor 변환 및 LSTM에 들어가도록 변환
- nn.LSTM은 (시퀀스 개수, 배치 개수, feature 개수 ) 순으로 데이터를 받음


```python
sequence_data_tensor = torch.tensor(sequence_dataX, dtype=torch.float32)
print(sequence_data_tensor.shape)
batch_added_tensor = sequence_data_tensor[0].unsqueeze(1)
print(batch_added_tensor.shape)
```

    torch.Size([724, 6, 4])
    torch.Size([6, 1, 4])
    

## 6. LSTM으로 시퀀스 데이터 처리


```python
tot_num_sequence_data = sequence_data_tensor.size(0)

for i in range(tot_num_sequence_data):
    # 현재 시퀀스 선택 및 차원 추가 (seq_len, batch, input_size)
    current_sequence = sequence_data_tensor[i].unsqueeze(1)  # 배치 차원 추가

    # LSTM 네트워크를 통한 시퀀스 처리
    output, (hn, cn) = model_lstm(current_sequence, (h0, c0))

 # 결과 출력 (첫 번째, 마지막, 매 100번째 시퀀스만 출력)
    if i == 0 or i == sequence_data_tensor.size(0) - 1 or i % 100 == 0:
        print(f"Sequence {i+1} Output shape:", output.shape)
        print(f"Sequence {i+1} Hidden state shape:", hn.shape)
        print(f"Sequence {i+1} Cell state shape:", cn.shape)
        
    h0, c0 = hn, cn 

```

    Sequence 1 Output shape: torch.Size([6, 1, 10])
    Sequence 1 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 1 Cell state shape: torch.Size([1, 1, 10])
    Sequence 101 Output shape: torch.Size([6, 1, 10])
    Sequence 101 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 101 Cell state shape: torch.Size([1, 1, 10])
    Sequence 201 Output shape: torch.Size([6, 1, 10])
    Sequence 201 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 201 Cell state shape: torch.Size([1, 1, 10])
    Sequence 301 Output shape: torch.Size([6, 1, 10])
    Sequence 301 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 301 Cell state shape: torch.Size([1, 1, 10])
    Sequence 401 Output shape: torch.Size([6, 1, 10])
    Sequence 401 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 401 Cell state shape: torch.Size([1, 1, 10])
    

    Sequence 501 Output shape: torch.Size([6, 1, 10])
    Sequence 501 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 501 Cell state shape: torch.Size([1, 1, 10])
    Sequence 601 Output shape: torch.Size([6, 1, 10])
    Sequence 601 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 601 Cell state shape: torch.Size([1, 1, 10])
    Sequence 701 Output shape: torch.Size([6, 1, 10])
    Sequence 701 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 701 Cell state shape: torch.Size([1, 1, 10])
    Sequence 724 Output shape: torch.Size([6, 1, 10])
    Sequence 724 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 724 Cell state shape: torch.Size([1, 1, 10])
    

## 복잡하게 Batch 신경 안 쓰려면 모듈 사용(TensorDataset)


```python
import torch
from torch.utils.data import TensorDataset, DataLoader

# 텐서로 변환된 시퀀스 데이터와 레이블
sequence_data_tensor = torch.tensor(sequence_dataX, dtype=torch.float32)
sequence_labels_tensor = torch.tensor(sequence_dataY, dtype=torch.float32)

# TensorDataset 생성
train_dataset = TensorDataset(sequence_data_tensor, sequence_labels_tensor)

# DataLoader 생성: 배치 크기를 정하고, 데이터 셋을 셔플
batch_size = 32  # 배치 크기 설정
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

total_num_batches = len(train_loader)
    
print("데이터셋 총 길이:", len(train_loader.dataset))
print("train_loader에 설정된 batch size:",train_loader.batch_size )  # 데이터셋의 총 샘플 수
print("num total batches in train_loader:",total_num_batches)   # 데이터셋의 총 샘플 

# 첫 번째 배치를 불러와서 데이터 형태 확인
first_batch = next(iter(train_loader))
data, target = first_batch
print(f"첫번째 배치 데이터 'data'의 형태 : {data.shape}")
print(f"첫번째 배치 데이터 'target' 형태 : {target.shape}")


```

    데이터셋 총 길이: 724
    train_loader에 설정된 batch size: 32
    num total batches in train_loader: 23
    첫번째 배치 데이터 'data'의 형태 : torch.Size([32, 6, 4])
    첫번째 배치 데이터 'target' 형태 : torch.Size([32])
    


```python
model_lstm = nn.LSTM(input_size=num_features, hidden_size=num_hidden, num_layers=1)

i=0
# 데이터 로더를 이용하여 배치 데이터 처리
for data, target in train_loader:
    # 동적으로 h0, c0 초기화
    batch_size_actual = data.size(0)  # 실제 배치 크기
    # data를 (batch_size, sequence_length, num_feature) 순서에서 (sequence_length, batch_size, num_feature) 순으로 변경 - LSTM에 넣으려고
    data = data.permute(1, 0, 2)
    
    h0 = torch.zeros(1, batch_size_actual, num_hidden)
    c0 = torch.zeros(1, batch_size_actual, num_hidden)

    # LSTM 네트워크를 통한 시퀀스 처리
    output, (hn, cn) = model_lstm(data, (h0, c0))
    
    if i== (total_num_batches-1) or i== (total_num_batches-2) or i== (total_num_batches-3):
        print(f"{i+1}th Batch data shape:{data.shape}")  # 각 배치의 데이터 형태
        print(f"{i+1}th Batch target shape: {target.shape}")  # 각 배치의 타겟 형태
        print(f"{i+1}th Output shape of batch: {output.shape}")  # 모델 출력 형태
        # 마지막 batch는 깔끔하게 안 떨어졌기 때문에 [6,20,4]로 나온다.
    i += 1
```

    21th Batch data shape:torch.Size([6, 32, 4])
    21th Batch target shape: torch.Size([32])
    21th Output shape of batch: torch.Size([6, 32, 10])
    22th Batch data shape:torch.Size([6, 32, 4])
    22th Batch target shape: torch.Size([32])
    22th Output shape of batch: torch.Size([6, 32, 10])
    23th Batch data shape:torch.Size([6, 20, 4])
    23th Batch target shape: torch.Size([20])
    23th Output shape of batch: torch.Size([6, 20, 10])
    



# 4. LSTM Model 만들기
- LSTM Cell을 포함하며 자동으로 만들어주도록 함


```python
import torch

# 난수 시드 설정
def set_seed(seed_value):
    random.seed(seed_value)       # 파이썬 난수 생성기
    np.random.seed(seed_value)    # Numpy 난수 생성기
    torch.manual_seed(seed_value) # PyTorch 난수 생성기

    # CUDA 환경에 대한 시드 설정 (GPU 사용 시)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
seed_value = 42
set_seed(seed_value) # 위에서 정의한 함수 호출로 모든 시드 설정
```




```python
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)  # 출력 크기 조정을 위한 선형 레이어

    def forward(self, x):
        batch_size = x.size(0)

        h0, c0 = self.init_hidden(batch_size)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:,-1,:])  # 마지막 타임 스텝의 출력만 사용
        return out
    
    def init_hidden(self, batch_size):
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        return h0, c0

# 모델 인스턴스 생성 및 사용
num_features = len(features)
num_hidden = 10
num_output = 1
model_lstm = LSTMModel(input_size=num_features, hidden_size=num_hidden, num_layers = 1,output_size=num_output)

model_lstm
```




    LSTMModel(
      (lstm): LSTM(4, 10, batch_first=True)
      (fc): Linear(in_features=10, out_features=1, bias=True)
    )



## 데이터 정규화


```python
from sklearn.preprocessing import StandardScaler

# 정규화를 위한 스케일러 초기화 및 적용
scaler = StandardScaler()
df_weather = df_weather.drop(["day"], axis = 1)
weather_scaled_arr = scaler.fit_transform(df_weather)

# DataFrame으로 변환 (스케일러는 numpy array를 반환하기 때문)
df_weather_scaled = pd.DataFrame(weather_scaled_arr, columns=features)

```


```python
# 시퀀스 길이 정의
seq_length = 6  # 과거 6일의 데이터를 기반으로 다음날의 기온을 예측
# 데이터셋 생성
sequence_dataX, sequence_dataY = build_sequence_dataset(df_weather_scaled, seq_length)
```


```python
from sklearn.model_selection import train_test_split 

# sequence_dataX와 sequence_dataY를 사용하여 데이터를 학습 세트와 테스트 세트로 분할
train_X, test_X, train_Y, test_Y = train_test_split(
    sequence_dataX, sequence_dataY, test_size=0.2, shuffle = False)

# 분할된 데이터의 크기 확인
print("학습 데이터 크기:", train_X.shape)
print("테스트 데이터 크기:", test_X.shape)
print(type(train_X))
```

    학습 데이터 크기: (579, 6, 4)
    테스트 데이터 크기: (145, 6, 4)
    <class 'numpy.ndarray'>
    


```python
import torch
from torch.utils.data import TensorDataset, DataLoader

# 텐서로 데이터 변환
train_X_tensor = torch.tensor(train_X, dtype=torch.float32)
train_Y_tensor = torch.tensor(train_Y.reshape(-1, 1), dtype=torch.float32)
test_X_tensor = torch.tensor(test_X, dtype=torch.float32)
test_Y_tensor = torch.tensor(test_Y.reshape(-1, 1), dtype=torch.float32)

# TensorDataset 생성
train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)
test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)

# DataLoader 설정
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 첫 번째 배치를 불러와서 데이터 형태 확인
first_batch = next(iter(train_loader))
data, target = first_batch

```

## 손실함수, Optim 설정


```python
import torch.optim as optim

learning_rate = 0.01
optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)
criterion_lstm = nn.MSELoss()

```

## train


```python
def train(model, train_loader, optimizer, criterion):
    model.train()  # 모델을 학습 모드로 설정
    total_loss = 0
    i = 0
    for data, target in train_loader:
        optimizer.zero_grad()
          
        output = model(data)
            
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        i+=1
    return total_loss / len(train_loader)

epoch_loss = train(model_lstm, train_loader, optimizer_lstm, criterion_lstm)
print(f"epoch loss(mse) = {epoch_loss:.4f}")

```

    epoch loss(mse) = 0.5331
    

## 여러 Epoch에 걸쳐 학습


```python
# 각 에포크의 평균 손실을 저장할 리스트 초기화
loss_history = []

max_epochs = 200
for epoch in range(max_epochs):
    epoch_loss = train(model_lstm, train_loader, optimizer_lstm, criterion_lstm)
    loss_history.append(epoch_loss)  # 손실 기록
    if (epoch+1) % 10 == 0:
        print(f"epoch {epoch+1}: loss(mse) = {epoch_loss:.4f}")

print(f"학습 완료 : 총 {epoch+1} epoch")

```

    epoch 10: loss(mse) = 0.0432
    epoch 20: loss(mse) = 0.0391
    epoch 30: loss(mse) = 0.0393
    epoch 40: loss(mse) = 0.0330
    epoch 50: loss(mse) = 0.0291
    epoch 60: loss(mse) = 0.0273
    epoch 70: loss(mse) = 0.0256
    epoch 80: loss(mse) = 0.0245
    epoch 90: loss(mse) = 0.0213
    epoch 100: loss(mse) = 0.0196
    epoch 110: loss(mse) = 0.0175
    epoch 120: loss(mse) = 0.0176
    epoch 130: loss(mse) = 0.0293
    epoch 140: loss(mse) = 0.0176
    epoch 150: loss(mse) = 0.0136
    epoch 160: loss(mse) = 0.0112
    epoch 170: loss(mse) = 0.0108
    epoch 180: loss(mse) = 0.0088
    epoch 190: loss(mse) = 0.0090
    epoch 200: loss(mse) = 0.0084
    학습 완료 : 총 200 epoch
    

## 그래프 시각화


```python
import matplotlib.pyplot as plt

# 에포크에 따른 손실 그래프 그리기
plt.figure(figsize=(8, 5))
plt.plot(loss_history, marker='o', linestyle='-', color='blue')
plt.title('Epoch vs. Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

```


![png](/assets/images/2025-02-04-DL9_47_0.png)
    


## 모델 검증


```python
def validate_model(model, test_loader, criterion):
    model.eval()  # 모델을 평가 모드로 설정
    total_loss = 0
    actuals = []
    predictions = []

    with torch.no_grad():  # 그라디언트 계산을 비활성화
        for data, target in test_loader:
            output = model(data)
            loss = criterion(output, target)
            total_loss += loss.item()

            # 예측값과 실제값을 리스트에 저장
            actuals.extend(target.squeeze(1).tolist())
            predictions.extend(output.squeeze(1).tolist())
            
    # 손실 계산
    avg_loss = total_loss / len(test_loader)
    
    return avg_loss, actuals, predictions

# 모델 검증 및 실제값, 예측값 가져오기
test_loss, actuals, predictions = validate_model(model_lstm, test_loader, criterion_lstm)
print(f"테스트 손실: {test_loss:.4f}")
print("Actuals Sample:", actuals[:10])
print("Predictions Sample:", predictions[:10])
```

    테스트 손실: 0.0651
    Actuals Sample: [1.079803228378296, 1.04249107837677, 1.0704751014709473, 1.2570358514785767, 1.145099401473999, 1.3130040168762207, 1.3596442937850952, 1.2290517091751099, 1.1637555360794067, 1.1264433860778809]
    Predictions Sample: [1.3201887607574463, 1.3294987678527832, 1.2368884086608887, 1.026627540588379, 1.233774185180664, 1.1806755065917969, 1.2663557529449463, 1.3628737926483154, 1.1398649215698242, 1.1686506271362305]
    

## 실제값과 예측값 시각화


```python
# 실제값과 예측값을 시각화
plt.figure(figsize=(8, 4))
plt.plot(actuals, label='Actual Values', color='blue')
plt.plot(predictions, label='Predicted Values', color='red', alpha=0.5)
plt.title('Actual vs Predicted Values')
plt.xlabel('Sample Number')
plt.ylabel('Temperature')
plt.legend()
plt.show()
```


![png](/assets/images/2025-02-04-DL9_51_0.png)
    

