---
layout: single
title:  "DL-15. Transformer(이론)"
---

# Transformer(이론)

## 1. Transformer 등장 배경
 1. 기존 RNN류들의 한계 : 입력 시퀀스를 순차적으로 처리해야하기 때문에, 병렬 처리가 어렵고 학습 속도가 느리다는 한계점 존재  
 2. LSTM, GRU도 매우 긴 문장이나, 복잡한 문장에서는 Gradient Vanishing 문제 발생

$\Rightarrow$ RNN류를 대체할 병렬 처리 및 Gradient Vanishing이 없는 새로운 구조가 필요함. **Transformer의 등장**  

Transformer는 **Self-Attention(Mutlti-Head Attention)**,  **Feed-Foward**라는 핵심 메커니즘 존재

![png](/assets/images/DL-15-image-2.png)

## 2. Self-Attention
=> **Attention을 자기 자신한테 취한다는 의미**  

Attention : 서로 다른 시퀀스(입력과 출력) 사이에서 정보를 연결하는 과정  
Self-Attention : 동일한 시퀀스 내에서 모든 단어가 서로를 참고하여 정보를 업데이트

Self Attention의 작동 방식
1. 입력 시퀀스가 토큰화
2. 각 토큰들은 임베딩을 통해 고정 차원의 벡터로 표현됨
3. 모든 벡터들(즉, 모든 단어들) 선형 변환을 통해 Q(Query), K(Key), V(Value) 3가지 벡터를 생성
    - Self-Attention는 각 단어가 다른 단어와 어떤 관계를 맺고 있는지 동적으로 계산해야함. 따라서, 각 단어를 세 가지 역할로 분류함
    - Q(Query) : "내가 무엇을 필요로 하는가?" -> Q 벡터는 어떤 정보를 요청하려고 업데이트하려는지 나타냄
    - K(Key) : "내가 무엇을 가지고 있는가?" -> K 벡터는 단어가 가진 특성을 나타냄. 다른 단어들의 Query와 자기 단어가 얼마나 연관되는지 평가할 때 사용
    - V(Value) : "내가 가진 정보는 무엇인가?" -> V 벡터는 Self-Attention에서 다른 단어들이 자기 단어를 참고할 때 사용하는 정보

$$ Q = XW^Q,\quad K = XW^K,\quad V = XW^V $$ 
 $X$  = 모든 단어들의 임베딩 벡터를 모은 행렬  
 $W^Q, W^K, W^V$ = 각각 Query, Key, Value를 위한 학습 가능한 가중치 행렬  
 $Q, K, V$ = Attention Score 및 상관관계에 사용 예정

4. Attention Score 계산
    1. 유사도 계산
        - 각 단어의 Query $q_i$와 모든 단어의 Key $k_j$ 사이의 내적을 계산
        - $score(i,j) = q_i \cdot k_j $ => 단어 i와 단어 k가 얼마나 관련이 있는지의 정도

    2. 스케일링(scaling)
        - 내적 값이 너무 커지는 걸 막기 위해 $\sqrt{d_k}$ 로 나눠줌. (대체적으로 $d_q = d_k$ 라서 $K$의 차원만큼 나눠줌)
        - $scaled\ score(i,j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$
    3. Softmax를 통한 정규화
        - Scaled score에 softmax 함수 적용
        - $\alpha_{ij} = softmax_j \left( \frac{q_i \cdot k_j}{\sqrt{d_k}} \right) = \frac{ \exp\left( \frac{q_i \cdot k_j}{\sqrt{d_k}} \right)}{\sum_{j'}\exp\left( \frac{q_i \cdot k_{j'}}{\sqrt{d_k}} \right)} $ = 단어 i가 단어 j에 대해 가지는 Attention Weight

5. Context Vector 계산
    - 아직 사용하지 않은 V 벡터와 Attention Weight를 가중 합해서 Context Vector 계산
    - $Context\ Vector_i = \underset{j}\sum \alpha_{ij} V_j$ 
    - 즉, 각 단어 i 마다 Context Vector를 계산해 이후에 활용

전체 과정을 나타낸 이미지  
![png](/assets/images/DL-15-image.png)

## 3. Multi-Head Attention
- 단어 간의 관계성을 좀 더 정확하고 다양하게 파악하기 위해서 Self-Attention을 여러 번 시행한다고 생각하면 됨.
- 입력 토큰의 임베딩 이후, 임베딩 벡터의 차원이 16이라고 가정해보자.

단일 head Self-Attention의 경우에는 Q, K, V의 차원도 다 16으로 설정되겠지만, Multi-Head Attention의 경우에는 $\frac{16}{head\ num}$로 결정  
각 head에서 Context Vector까지 알아서 계산한 후, 단순하게 직렬로 합쳐버림(Concat 과정)  

$head_i = ATTENTION({\mathbf {QW} }_{q,i} , {\mathbf {KW} }_{k,i}, {\mathbf {VW} }_{v,i} )$  
$ MultiHeadAttention (Q, K, V) = Concat(head_1, ..., head_h )W^O $,   $\quad W^O$ = 학습 가능한 가중치 행렬 


## 4. Feed-Foward Network

![png](/assets/images/DL-15-image-3.png)

Self-Attention(혹은 MultiHead-Attention) 뒤에 등장하며, Attention을 통해 구한 Context Vector에 비선형성을 추가해서 더 복잡하고 풍부한 특성을 학습할 수 있도록 만든다.



$$ FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2$$

$x$ : Context Vector, 단어에 대한 문맥 정보  
$W_1, W_2, b_1, b_2$ : Linear Transformation을 위해 사용되는 학습 가능한 가중치 행렬 및 bias

- x를 한번 차원을 키웠다가 다시 줄임으로써 좀 더 중요한 정보만 다루도록 수정하는 과정

## 5. 부가적인 요소

1. Residual Connection
    - SubLayer(Self-Attention, FFN)을 거치면서 생길 수 있는 정보 손실을 방지
    - 수식으로는 $Output = x + Sublayer(x)$, 단순히 Sublayer(x) 결과를 활용하는 게 아닌, 급격한 변화를 방지하기 위해 합쳐서 전달
2. Layer Normalization 
    - Residual Connection 이후 전체 Layer에 대한 정규화 과정
    - 정규화를 통해 모델이 좀 더 안정적인 학습 가능
    - 입력으로 들어온 모든 Feature 내부의 스칼라들 $s_i$를 이용해서 평균 $\mu_i$와 표준편차 $\sigma_i^2$을 계산
    - 그리고 Normalization 진행 $\hat{s_i} = \alpha_i \frac{s_i - \mu_i}{\sqrt{ \sigma_i^2 + \epsilon}} + \beta_i$
    - 단순 Normalization만 진행하는게 아닌 학습 가능한 parameter $\alpha, \beta$를 도입해 Scaling, Shifting도 진행  

Residual Connection을 Add, Layer Normalization을 Norm으로 해서 둘이 합쳐 Add & Norm으로 표현한다.  

3. Positional Encoding 
    - 입력 토큰에 순서 정보를 넣어주기 위해서 사용. LSTM이나 GRU처럼 순서대로 처리해주는 것도 아니기 때문에 단어들의 위치 정보를 삽입할 필요가 있음
    - Transformer 논문에서는 고정된 함수를 사용했지만, 학습 가능한 파라미터로도 설정 가능
     $$ PE_{(pos, 2i)} = \sin \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right ) \quad PE_{(pos, 2i+1)} = \cos\left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right)$$
    pos : 토큰의 위치 (0, 1, 2, 3, ...)   
    i = 임베딩 차원의 인덱스 ( 임베딩 벡터의 홀수번째 인덱스에 해당하면 cos함수로, 짝수번째 인덱스에 해당하면 sin함수 )  
    $d_{model}$ = 임베딩 벡터의 차원  
    - 이런 위치 정보 PE에 Embedding을 더해서 Transformer의 Input으로 설정


## 6. Transformer

![png](/assets/images/DL-15-image-4.png)

**배운 걸 다 조합하면 Transformer 구조가 된다**

전반적인 Transformer 순서
1. Input Sequence를 토큰화 및 임베딩을 진행한다.
2. Positional Encoding을 통해서 임베딩 벡터에 위치 정보를 추가해준다.
3. Multi-Head Attention(Self-Attention)을 통해서 각 단어에 대해 다른 단어들에 대한 관계성을 계산한다.
4. Add&Norm을 통해 모델 학습 안정성을 증가시킨다.
5. FFNN을 통해서 동일하게 학습 안정성을 증가시킨다. 이후 Add&Norm까지 추가

여기까지가 현재 사용되는 Transformer의 정석적인 구조라고 생각한다. 논문 "Attention is All You Need"의 경우에는 Seq2Seq 형식, 즉 기계 번역처럼 입력 시퀀스 - 출력 시퀀스의 쌍을 이루기 위해서 인코더 - 디코더 형식으로 구성되어 있기 때문에 이걸 독립적인 2줄로 진행된다고 생각하면 되겠다. 논문 속 Transformer에서는 Seq2Seq처럼 인코더를 거친 값을 다시 디코더에 전달해주고 이를 바탕으로 다음 단어를 예측한다고 보면 된다.

Transformer의 확장 및 활용
1. Encoder-Only : BERT, RoBERTa, ALBERT         => 문장 이해
2. Decoder-Only : GPT-3, ChatGPT, LLaMA         => 텍스트 생성
3. Encoder-Decoder : Transformer(원본), T5, BART => 기계 번역, 문서 요약, 데이터 변환

다음에는 구현으로 진행한다
