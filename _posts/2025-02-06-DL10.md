---
layout: single
title:  "DL-10. GRU"
---

# 1. GRU이란?


## 1. GRU 등장한 이유
- LSTM과 마찬가지로, RNN의 graident vanishing 문제를 해결하기 위해 등장했다.
- LSTM보다 게이트를 덜 쓰며, Cell-state도 따로 없어 구조가 훨씬 단순하지만, 성능이 유사하기 때문에 매우 효율적인 모델이다.

## 2. GRU 구조

![png](/assets/images/2025-02-06-DL10_image.png)


1. Hidden-state(h)
    - 시점별로 Hidden-state가 존재해서, 과거 시점의 Hidden-state를 이용해 현재 시점의 Hidden-state 설정 
    - c (그림에선 $\tilde{H_t}$) = 다음 은닉후보 값을 의미한다
    - 다음 Hidden-state 값은 이전 Hidden-state와 Hidden-state 후보를 가중치로 합해서 결정된다

2. Gate 구조 = GRU는 Update, Reset이라는 2개의 Gate만 사용했다.
    - Update Gate(z) = 현재 입력과 이전 Hidden-State에 의해서 결정된다
        - z = 이전 은닉 상태를 얼마나 유지할지 결정한다. 1 - z = 은닉후보 값(c)를 얼마나 반영할지 결정
    - Reset Gate(r) = z와 동일한 방식으로 계산됨
3. 수식 정리  
    Update Gate : $z_t = \sigma(W_{xz}x_t + W_{hz}h_{t-1}+b_z)$  
    Reset Gate : $r_t = \sigma(W_{xr}x_t+ W_{hr}h_{t-1})$  
    Candidate : $c_t = tanh(W_{xg}x_t + W_{hg}(r_t\odot h_{t-1}) + b_g) $  
    Hidden State : $h_t = z_t\odot h_{t-1} + (1-z_t)\odot c_t$


# 2. LSTM Cell state 만들기
- 큰 맥락은 GRU나 LSTM이나 동일하다. nn.LSTM을 nn.GRU로 바꿔주기만 하면 됨.

## 1. 데이터 불러오기 + 전처리


```python
import numpy as np
import pandas as pd 
import random 
import datetime
train_data = pd.read_csv('C:\\스터디\\DACON\\시계열\\data\\train.csv')
features = ['humidity','rainfall','wspeed','temperature'] 
dict ={
    '평균습도' : 'humidity',
    '강수량' : 'rainfall',
    '평균풍속' : 'wspeed',
    '평균기온' : 'temperature',
    '일시' : 'day'
}
train_data = train_data[['일시', '평균습도', '강수량', '평균풍속', '평균기온']]
train_data = train_data.rename(columns = dict)
train_data['day'] = pd.to_datetime(train_data['day'])
train_data['rainfall'] = train_data['rainfall'].interpolate(method='linear', limit_direction='both')
df_weather = train_data[train_data['day'] >= '2021-01-01']
df_weather.reset_index(drop=True)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>day</th>
      <th>humidity</th>
      <th>rainfall</th>
      <th>wspeed</th>
      <th>temperature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-01-01</td>
      <td>64.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-4.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-01-02</td>
      <td>38.5</td>
      <td>0.0</td>
      <td>2.6</td>
      <td>-5.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-01-03</td>
      <td>45.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-5.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-01-04</td>
      <td>51.4</td>
      <td>0.0</td>
      <td>1.7</td>
      <td>-3.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-01-05</td>
      <td>52.8</td>
      <td>0.0</td>
      <td>2.9</td>
      <td>-5.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>725</th>
      <td>2022-12-27</td>
      <td>69.8</td>
      <td>0.6</td>
      <td>1.8</td>
      <td>-2.6</td>
    </tr>
    <tr>
      <th>726</th>
      <td>2022-12-28</td>
      <td>58.1</td>
      <td>0.1</td>
      <td>2.5</td>
      <td>-3.3</td>
    </tr>
    <tr>
      <th>727</th>
      <td>2022-12-29</td>
      <td>56.3</td>
      <td>0.0</td>
      <td>1.7</td>
      <td>-2.9</td>
    </tr>
    <tr>
      <th>728</th>
      <td>2022-12-30</td>
      <td>65.6</td>
      <td>0.0</td>
      <td>1.9</td>
      <td>-1.8</td>
    </tr>
    <tr>
      <th>729</th>
      <td>2022-12-31</td>
      <td>65.5</td>
      <td>0.0</td>
      <td>1.4</td>
      <td>-1.2</td>
    </tr>
  </tbody>
</table>
<p>730 rows × 5 columns</p>
</div>



## 2. 데이터를 시퀀스 데이터로 바꿔주기


```python
# 시퀀스 데이터 생성 함수
def build_sequence_dataset(df, seq_length):
    dataX = []
    dataY = []
    for i in range(0, len(df) - seq_length):
        _x = df.iloc[i:i + seq_length].values  # 시퀀스 데이터
        _y = df.iloc[i + seq_length]['temperature']  # 다음 포인트의 기온을 레이블로 사용
        dataX.append(_x)
        dataY.append(_y)
    return np.array(dataX), np.array(dataY)

# 시퀀스 길이 정의
seq_length = 6  # 과거 6일의 데이터를 기반으로 다음날의 기온을 예측
# 데이터셋 생성
sequence_dataX, sequence_dataY = build_sequence_dataset(df_weather[features], seq_length)

# 생성된 시퀀스 데이터의 첫 번째 요소를 출력하여 실습 예제 확인
print(f"sequence_dataX 형태 : {sequence_dataX.shape}")
print(f"sequence_dataX 갯수 : {len(sequence_dataX)}")

print("첫번째 sequenceX:", sequence_dataX[0])
print("첫번째 sequenceY:", sequence_dataY[0])

```

    sequence_dataX 형태 : (724, 6, 4)
    sequence_dataX 갯수 : 724
    첫번째 sequenceX: [[64.   0.   2.  -4.2]
     [38.5  0.   2.6 -5. ]
     [45.   0.   2.  -5.6]
     [51.4  0.   1.7 -3.5]
     [52.8  0.   2.9 -5.5]
     [54.6  2.3  2.4 -7.4]]
    첫번째 sequenceY: -14.5
    

## 3. GRU 모델 생성


```python
import torch
import torch.nn as nn

num_features = len(features)
num_hidden = 10

# GRU 모듈 초기화
model_gru = nn.GRU(input_size=num_features, hidden_size=num_hidden, num_layers=1)
print("모델 구조:")
print(model_gru)

for name, param in model_gru.named_parameters():
    print(f"{name}: {param.shape}")
```

    모델 구조:
    GRU(4, 10)
    weight_ih_l0: torch.Size([30, 4])
    weight_hh_l0: torch.Size([30, 10])
    bias_ih_l0: torch.Size([30])
    bias_hh_l0: torch.Size([30])
    

## 4. 가중치 초기화
- nn.GRU는 은닉 상태를 (레이어 개수, 배치 개수, 은닉 유닛 개수)로 지정


```python
h0 = torch.zeros(1, 1, num_hidden)

print("Hidden state shape:", h0.shape)

```

    Hidden state shape: torch.Size([1, 1, 10])
    

## 5. 시퀀스 데이터 Tensor 변환 및 GRU에 들어가도록 변환
- nn.GRU은 (시퀀스 개수, 배치 개수, feature 개수 ) 순으로 데이터를 받음


```python
sequence_data_tensor = torch.tensor(sequence_dataX, dtype=torch.float32)
print(sequence_data_tensor.shape)
batch_added_tensor = sequence_data_tensor[0].unsqueeze(1)
print(batch_added_tensor.shape)
```

    torch.Size([724, 6, 4])
    torch.Size([6, 1, 4])
    

## 6. GRU으로 시퀀스 데이터 처리


```python
tot_num_sequence_data = sequence_data_tensor.size(0)

for i in range(tot_num_sequence_data):
    # 현재 시퀀스 선택 및 차원 추가 (seq_len, batch, input_size)
    current_sequence = sequence_data_tensor[i].unsqueeze(1)  # 배치 차원 추가

    # GRU 네트워크를 통한 시퀀스 처리
    output, hn = model_gru(current_sequence, h0)

 # 결과 출력 (첫 번째, 마지막, 매 100번째 시퀀스만 출력)
    if i == 0 or i == sequence_data_tensor.size(0) - 1 or i % 100 == 0:
        print(f"Sequence {i+1} Output shape:", output.shape)
        print(f"Sequence {i+1} Hidden state shape:", hn.shape)
        
    h0 = hn

```

    Sequence 1 Output shape: torch.Size([6, 1, 10])
    Sequence 1 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 101 Output shape: torch.Size([6, 1, 10])
    Sequence 101 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 201 Output shape: torch.Size([6, 1, 10])
    Sequence 201 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 301 Output shape: torch.Size([6, 1, 10])
    Sequence 301 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 401 Output shape: torch.Size([6, 1, 10])
    Sequence 401 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 501 Output shape: torch.Size([6, 1, 10])
    Sequence 501 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 601 Output shape: torch.Size([6, 1, 10])
    Sequence 601 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 701 Output shape: torch.Size([6, 1, 10])
    Sequence 701 Hidden state shape: torch.Size([1, 1, 10])
    Sequence 724 Output shape: torch.Size([6, 1, 10])
    Sequence 724 Hidden state shape: torch.Size([1, 1, 10])
    

## 복잡하게 Batch 신경 안 쓰려면 모듈 사용(TensorDataset)


```python
import torch
from torch.utils.data import TensorDataset, DataLoader

# 텐서로 변환된 시퀀스 데이터와 레이블
sequence_data_tensor = torch.tensor(sequence_dataX, dtype=torch.float32)
sequence_labels_tensor = torch.tensor(sequence_dataY, dtype=torch.float32)

# TensorDataset 생성
train_dataset = TensorDataset(sequence_data_tensor, sequence_labels_tensor)

# DataLoader 생성: 배치 크기를 정하고, 데이터 셋을 셔플
batch_size = 32  # 배치 크기 설정
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

total_num_batches = len(train_loader)
    
print("데이터셋 총 길이:", len(train_loader.dataset))
print("train_loader에 설정된 batch size:",train_loader.batch_size )  # 데이터셋의 총 샘플 수
print("num total batches in train_loader:",total_num_batches)   # 데이터셋의 총 샘플 

# 첫 번째 배치를 불러와서 데이터 형태 확인
first_batch = next(iter(train_loader))
data, target = first_batch
print(f"첫번째 배치 데이터 'data'의 형태 : {data.shape}")
print(f"첫번째 배치 데이터 'target' 형태 : {target.shape}")


```

    데이터셋 총 길이: 724
    train_loader에 설정된 batch size: 32
    num total batches in train_loader: 23
    첫번째 배치 데이터 'data'의 형태 : torch.Size([32, 6, 4])
    첫번째 배치 데이터 'target' 형태 : torch.Size([32])
    


```python
model_gru = nn.GRU(input_size=num_features, hidden_size=num_hidden, num_layers=1)

i=0
# 데이터 로더를 이용하여 배치 데이터 처리
for data, target in train_loader:
    # 동적으로 h0, c0 초기화
    batch_size_actual = data.size(0)  # 실제 배치 크기
    data = data.permute(1, 0, 2)
    
    h0 = torch.zeros(1, batch_size_actual, num_hidden)

    # GRU 네트워크를 통한 시퀀스 처리
    output, hn = model_gru(data,h0 )
    
    if i== (total_num_batches-1) or i== (total_num_batches-2) or i== (total_num_batches-3):
        print(f"{i+1}th Batch data shape:{data.shape}")  # 각 배치의 데이터 형태
        print(f"{i+1}th Batch target shape: {target.shape}")  # 각 배치의 타겟 형태
        print(f"{i+1}th Output shape of batch: {output.shape}")  # 모델 출력 형태
        # 마지막 batch는 깔끔하게 안 떨어졌기 때문에 [6,20,4]로 나온다.
    i += 1
```

    21th Batch data shape:torch.Size([6, 32, 4])
    21th Batch target shape: torch.Size([32])
    21th Output shape of batch: torch.Size([6, 32, 10])
    22th Batch data shape:torch.Size([6, 32, 4])
    22th Batch target shape: torch.Size([32])
    22th Output shape of batch: torch.Size([6, 32, 10])
    23th Batch data shape:torch.Size([6, 20, 4])
    23th Batch target shape: torch.Size([20])
    23th Output shape of batch: torch.Size([6, 20, 10])
    

# 3. GRU Model 만들기
- GRU Cell을 포함하며 자동으로 만들어주도록 함


```python
import torch

# 난수 시드 설정
def set_seed(seed_value):
    random.seed(seed_value)       # 파이썬 난수 생성기
    np.random.seed(seed_value)    # Numpy 난수 생성기
    torch.manual_seed(seed_value) # PyTorch 난수 생성기

    # CUDA 환경에 대한 시드 설정 (GPU 사용 시)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
seed_value = 42
set_seed(seed_value) # 위에서 정의한 함수 호출로 모든 시드 설정
```


```python
import torch
import torch.nn as nn

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(GRUModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)  # 출력 크기 조정을 위한 선형 레이어

    def forward(self, x):
        batch_size = x.size(0)

        h0= self.init_hidden(batch_size)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:,-1,:])  # 마지막 타임 스텝의 출력만 사용
        return out
    
    def init_hidden(self, batch_size):
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        return h0

# 모델 인스턴스 생성 및 사용
num_features = len(features)
num_hidden = 10
num_output = 1
model_gru = GRUModel(input_size=num_features, hidden_size=num_hidden, num_layers = 1,output_size=num_output)

model_gru
```




    GRUModel(
      (gru): GRU(4, 10, batch_first=True)
      (fc): Linear(in_features=10, out_features=1, bias=True)
    )



## 데이터 정규화


```python
from sklearn.preprocessing import StandardScaler

# 정규화를 위한 스케일러 초기화 및 적용
scaler = StandardScaler()
df_weather = df_weather.drop(["day"], axis = 1)
weather_scaled_arr = scaler.fit_transform(df_weather)

# DataFrame으로 변환 (스케일러는 numpy array를 반환하기 때문)
df_weather_scaled = pd.DataFrame(weather_scaled_arr, columns=features)

```


```python
# 시퀀스 길이 정의
seq_length = 6  # 과거 6일의 데이터를 기반으로 다음날의 기온을 예측
# 데이터셋 생성
sequence_dataX, sequence_dataY = build_sequence_dataset(df_weather_scaled, seq_length)
```


```python
from sklearn.model_selection import train_test_split 

# sequence_dataX와 sequence_dataY를 사용하여 데이터를 학습 세트와 테스트 세트로 분할
train_X, test_X, train_Y, test_Y = train_test_split(
    sequence_dataX, sequence_dataY, test_size=0.2, shuffle = False)

# 분할된 데이터의 크기 확인
print("학습 데이터 크기:", train_X.shape)
print("테스트 데이터 크기:", test_X.shape)
print(type(train_X))
```

    학습 데이터 크기: (579, 6, 4)
    테스트 데이터 크기: (145, 6, 4)
    <class 'numpy.ndarray'>
    


```python
import torch
from torch.utils.data import TensorDataset, DataLoader

# 텐서로 데이터 변환
train_X_tensor = torch.tensor(train_X, dtype=torch.float32)
train_Y_tensor = torch.tensor(train_Y.reshape(-1, 1), dtype=torch.float32)
test_X_tensor = torch.tensor(test_X, dtype=torch.float32)
test_Y_tensor = torch.tensor(test_Y.reshape(-1, 1), dtype=torch.float32)

# TensorDataset 생성
train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)
test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)

# DataLoader 설정
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 첫 번째 배치를 불러와서 데이터 형태 확인
first_batch = next(iter(train_loader))
data, target = first_batch

```

## 손실함수, Optim 설정


```python
import torch.optim as optim

learning_rate = 0.01
optimizer_gru = torch.optim.Adam(model_gru.parameters(), lr=learning_rate)
criterion_gru = nn.MSELoss()

```

## train


```python
def train(model, train_loader, optimizer, criterion):
    model.train()  # 모델을 학습 모드로 설정
    total_loss = 0
    i = 0
    for data, target in train_loader:
        optimizer.zero_grad()
          
        output = model(data)
            
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        i+=1
    return total_loss / len(train_loader)

epoch_loss = train(model_gru, train_loader, optimizer_gru, criterion_gru)
print(f"epoch loss(mse) = {epoch_loss:.4f}")

```

    epoch loss(mse) = 0.4743
    

## 여러 Epoch에 걸쳐 학습


```python
# 각 에포크의 평균 손실을 저장할 리스트 초기화
loss_history = []

max_epochs = 200
for epoch in range(max_epochs):
    epoch_loss = train(model_gru, train_loader, optimizer_gru, criterion_gru)
    loss_history.append(epoch_loss)  # 손실 기록
    if (epoch+1) % 10 == 0:
        print(f"epoch {epoch+1}: loss(mse) = {epoch_loss:.4f}")

print(f"학습 완료 : 총 {epoch+1} epoch")

```

    epoch 10: loss(mse) = 0.0467
    epoch 20: loss(mse) = 0.0406
    epoch 30: loss(mse) = 0.0334
    epoch 40: loss(mse) = 0.0349
    epoch 50: loss(mse) = 0.0308
    epoch 60: loss(mse) = 0.0280
    epoch 70: loss(mse) = 0.0261
    epoch 80: loss(mse) = 0.0220
    epoch 90: loss(mse) = 0.0213
    epoch 100: loss(mse) = 0.0180
    epoch 110: loss(mse) = 0.0181
    epoch 120: loss(mse) = 0.0156
    epoch 130: loss(mse) = 0.0159
    epoch 140: loss(mse) = 0.0135
    epoch 150: loss(mse) = 0.0117
    epoch 160: loss(mse) = 0.0115
    epoch 170: loss(mse) = 0.0121
    epoch 180: loss(mse) = 0.0099
    epoch 190: loss(mse) = 0.0091
    epoch 200: loss(mse) = 0.0131
    학습 완료 : 총 200 epoch
    

## 그래프 시각화


```python
import matplotlib.pyplot as plt

# 에포크에 따른 손실 그래프 그리기
plt.figure(figsize=(8, 5))
plt.plot(loss_history, marker='o', linestyle='-', color='blue')
plt.title('GRU Epoch vs. Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

```


![png](/assets/images/2025-02-06-DL10_36_0.png)
    


## 모델 검증


```python
def validate_model(model, test_loader, criterion):
    model.eval()  # 모델을 평가 모드로 설정
    total_loss = 0
    actuals = []
    predictions = []

    with torch.no_grad():  # 그라디언트 계산을 비활성화
        for data, target in test_loader:
            output = model(data)
            loss = criterion(output, target)
            total_loss += loss.item()

            # 예측값과 실제값을 리스트에 저장
            actuals.extend(target.squeeze(1).tolist())
            predictions.extend(output.squeeze(1).tolist())
            
    # 손실 계산
    avg_loss = total_loss / len(test_loader)
    
    return avg_loss, actuals, predictions

# 모델 검증 및 실제값, 예측값 가져오기
test_loss, actuals, predictions = validate_model(model_gru, test_loader, criterion_gru)
print(f"테스트 손실: {test_loss:.4f}")
print("Actuals Sample:", actuals[:10])
print("Predictions Sample:", predictions[:10])
```

    테스트 손실: 0.0872
    Actuals Sample: [1.079803228378296, 1.04249107837677, 1.0704751014709473, 1.2570358514785767, 1.145099401473999, 1.3130040168762207, 1.3596442937850952, 1.2290517091751099, 1.1637555360794067, 1.1264433860778809]
    Predictions Sample: [1.0593388080596924, 1.169521689414978, 1.3330940008163452, 1.1846405267715454, 1.376297116279602, 1.1462416648864746, 1.2843942642211914, 0.9538298845291138, 0.7834171652793884, 0.8921544551849365]
    

## 실제값과 예측값 시각화


```python
# 실제값과 예측값을 시각화
plt.figure(figsize=(8, 4))
plt.plot(actuals, label='Actual Values', color='blue')
plt.plot(predictions, label='Predicted Values', color='red', alpha=0.5)
plt.title('Actual vs Predicted Values')
plt.xlabel('Sample Number')
plt.ylabel('Temperature')
plt.legend()
plt.show()
```

![png](/assets/images/2025-02-06-DL10_40_0.png)
    

