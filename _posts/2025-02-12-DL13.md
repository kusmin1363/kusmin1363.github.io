---
layout: single
title:  "DL-13. Attention(구현 1)"
---

# Attention Mechanism - 구현 1

## 1. Vocabulary 정의 및 문장-pair
  Vocabulary는 언어별로 따로 정의


```python
# 영어 단어장 정의
vocab_en = {
    # 특수 토큰
    "<sos>": 0,
    "<eos>": 1,
    "she": 2, "is": 3, "reading": 4, "a": 5, "book": 6,
    "he": 7, "they": 8, "writing": 9, "the": 10,
    "letter": 11, "newspaper": 12
}

# 한국어 단어장 정의
vocab_ko = {
    # 특수 토큰
    "<sos>": 0,
    "<eos>": 1,
    "그녀는": 2, "그는": 3, "그들은": 4,
    "읽고": 5, "쓰고": 6,
    "있다": 7,
    "책을": 8, "편지를": 9, "신문을": 10
}

# 예시 문장 쌍 정의
train_pairs = [
    ("she is reading a book", "그녀는 책을 읽고 있다"),
    ("he is reading a newspaper", "그는 신문을 읽고 있다"),
    ("she is writing a letter", "그녀는 편지를 쓰고 있다"),
]

# 첫 번째 예시 문장에 대한 처리 예시
source_sentence = train_pairs[0][0]  # "she is reading a book"
target_sentence = train_pairs[0][1]  # "그녀는 책을 읽고 있다"  

print(f"영어 어휘 크기: {len(vocab_en)}")
print(f"한국어 어휘 크기: {len(vocab_ko)}")
print("\n입력 문장:", source_sentence)
print("목표 문장:", target_sentence)

print("\n예시 문장 인덱스 처리:")
# 공백 기준으로 자르고, vocab_en를 확인해 인덱스로 변경
print("영어:", [(word, vocab_en[word]) for word in source_sentence.split()])
print("한국어:", [(word, vocab_ko[word]) for word in target_sentence.split()])

```

    영어 어휘 크기: 13
    한국어 어휘 크기: 11
    
    입력 문장: she is reading a book
    목표 문장: 그녀는 책을 읽고 있다
    
    예시 문장 인덱스 처리:
    영어: [('she', 2), ('is', 3), ('reading', 4), ('a', 5), ('book', 6)]
    한국어: [('그녀는', 2), ('책을', 8), ('읽고', 5), ('있다', 7)]
    5
    

## 2. 문장을 인덱스 변환하는 함수 제작


```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 재현성을 위한 시드 설정
torch.manual_seed(42)

def sentence_to_indices(sentence: str, vocab: dict) -> torch.Tensor:
    # 소문자화 + 공백 자르기
    words = sentence.lower().split()

    # vocab에 들어있는 기준으로 인덱스로 변경
    indices = [vocab["<sos>"]] + [vocab[word] for word in words] + [vocab["<eos>"]]
    
    # 배치 차원 추가 ([sequence_length] -> [batch_size=1, sequence_length])
    return torch.tensor(indices).unsqueeze(0)

# 입력 문장 처리
src_tensor = sentence_to_indices(source_sentence, vocab_en)
tgt_tensor = sentence_to_indices(target_sentence, vocab_ko)

print("=== 문장 인덱스 변환 및 텐서 생성 결과 ===\n")

print("1. 소스:")
print(f"  실제 문장:{source_sentence}")
print(f"  인덱스 화:{src_tensor}")
print(f"  Shape: {src_tensor.shape}\n")

print("2. 타겟:")
print(f"  실제 문장:{target_sentence}")
print(f"  인덱스 화:{tgt_tensor}")
print(f"  Shape: {tgt_tensor.shape}\n")

```

    === 문장 인덱스 변환 및 텐서 생성 결과 ===
    
    1. 소스:
      실제 문장:she is reading a book
      인덱스 화:tensor([[0, 2, 3, 4, 5, 6, 1]])
      Shape: torch.Size([1, 7])
    
    2. 타겟:
      실제 문장:그녀는 책을 읽고 있다
      인덱스 화:tensor([[0, 2, 8, 5, 7, 1]])
      Shape: torch.Size([1, 6])
    
    

## 3. 임베딩 레이어 정의 및 임베딩


```python
import torch.nn as nn  

embedding_dim = 8
hidden_dim = 8

# 임베딩 레이어 정의
encoder_embedding = nn.Embedding(len(vocab_en), embedding_dim)
decoder_embedding = nn.Embedding(len(vocab_ko), embedding_dim)

# 소스 시퀀스 임베딩
src_embedded = encoder_embedding(src_tensor)


print("1. 임베딩 레이어 정보:")
print(f"  Encoder Embedding: {encoder_embedding}")
print(f"  Decoder Embedding: {decoder_embedding}\n")

print("2. 임베딩 진행:")
print(f"  임베딩 전 Shape: {src_tensor.shape}")
print(f"  임베딩 후 Shape: {src_embedded.shape}")
```

    1. 임베딩 레이어 정보:
      Encoder Embedding: Embedding(13, 8)
      Decoder Embedding: Embedding(11, 8)
    
    2. 임베딩 진행:
      임베딩 전 Shape: torch.Size([1, 7])
      임베딩 후 Shape: torch.Size([1, 7, 8])
    

## 4. Encoder 정의 및 LSTM 출력 확인
- 벡터 노름 : 정보량의 지표로서 클 수록 중요한 정보임


```python
# LSTM 엔코더 정의
encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)

# 전체 시퀀스 인코딩
encoder_outputs, (encoder_hidden, encoder_cell) = encoder(src_embedded)
print(f"  인코딩 후 Output Shape: {encoder_outputs.shape}")
print(f"  인코딩 후 Hidden Shape: {encoder_hidden.shape}")
print(f"  인코딩 후 Cell Shape: {encoder_cell.shape}")

print("각 단어별 엔코더 출력:")
for i, (word, output) in enumerate(zip(source_sentence.split() + ['<eos>'], encoder_outputs[0])):
    print(f"\n단어: {word}")
    print(f"벡터 노름: {torch.norm(output).item():.4f}")

```

      인코딩 후 Output Shape: torch.Size([1, 7, 8])
      인코딩 후 Hidden Shape: torch.Size([1, 1, 8])
      인코딩 후 Cell Shape: torch.Size([1, 1, 8])
    각 단어별 엔코더 출력:
    
    단어: she
    벡터 노름: 0.3415
    
    단어: is
    벡터 노름: 0.2573
    
    단어: reading
    벡터 노름: 0.4670
    
    단어: a
    벡터 노름: 0.3665
    
    단어: book
    벡터 노름: 0.3606
    
    단어: <eos>
    벡터 노름: 0.6286
    

## 5. 디코더 초기 설정 - Attention Layer : Badhanau
Badhanau Style이라 encoder_hidden_dim + decoder_hidden_dim의 크기만큼 합침.  
근데 둘 다 같은 hidden_dim으로 그냥 *2로 씀

 $e_{ij} = v_a^\top tanh(W_a[s_{i-1}; h_j])$


```python
# 디코더 초기 입력 설정
first_decoder_input = tgt_tensor[:, 0].unsqueeze(1)  # <sos> 토큰으로 시작
first_decoder_embedded = decoder_embedding(first_decoder_input)

# Badhanau Style이라 차원 더해진만큼이 input으로
decoder = nn.LSTM(embedding_dim+hidden_dim, hidden_dim, batch_first=True)

# 출력층 정의 (결합된 출력을 vocabulary 크기로 변환)
output_layer = nn.Linear(hidden_dim*2, len(vocab_ko))

# Attention 관련 레이어 정의
# W_a 부분
attn_projection_layer = nn.Linear(hidden_dim*2, hidden_dim)
# v_a 부분
attention_v = nn.Linear(hidden_dim,1, bias=False)

# 디코더 초기 상태 설정
decoder_hidden = encoder_hidden
decoder_cell = encoder_cell
decoder_input = first_decoder_input  
```


```python
print("\n=== 디코더 초기 설정 및 레이어 정의 결과 ===")

print("1. 디코더 초기 입력 정보:")
print(f"  입력 텐서 shape: {first_decoder_input.shape}")
print(f"  입력 텐서 내용: {first_decoder_input}")
print(f"  임베딩 결과 shape: {first_decoder_embedded.shape}")

print(f"\n2. 디코더 LSTM 구조: {decoder}")

print(f"\n3. 출력층 정보: {output_layer}")
print(f"  입력 크기: {hidden_dim * 2}")
print(f"  출력 크기: {len(vocab_ko)}")

print("\n4. 어텐션 레이어 정보")
print(f"  Projection Layer: {attn_projection_layer}")
print(f"  Score Layer: {attention_v}")

print("\n5. 디코더 초기 상태:")
print(f"  hidden state shape: {decoder_hidden.shape}")
print(f"  cell state shape: {decoder_cell.shape}")
print(f"  input shape: {decoder_input.shape}")
```

    
    === 디코더 초기 설정 및 레이어 정의 결과 ===
    1. 디코더 초기 입력 정보:
      입력 텐서 shape: torch.Size([1, 1])
      입력 텐서 내용: tensor([[0]])
      임베딩 결과 shape: torch.Size([1, 1, 8])
    
    2. 디코더 LSTM 구조: LSTM(16, 8, batch_first=True)
    
    3. 출력층 정보: Linear(in_features=16, out_features=11, bias=True)
      입력 크기: 16
      출력 크기: 11
    
    4. 어텐션 레이어 정보
      Projection Layer: Linear(in_features=16, out_features=8, bias=True)
      Score Layer: Linear(in_features=8, out_features=1, bias=False)
    
    5. 디코더 초기 상태:
      hidden state shape: torch.Size([1, 1, 8])
      cell state shape: torch.Size([1, 1, 8])
      input shape: torch.Size([1, 1])
    

## 6. Attention Score, Weight, Context Vector 계산
 $e_{ij} = v_a^\top tanh(W_a[s_{i-1}; h_j])$ 에 따라서 tanh 처리하고, $v_a$와도 연산    
 연산 값은 logit 값(raw 값)이기 때문에 음수도 가능, Softmax까지 해줘야 0~1 사이 값 나옴  

 $\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^L exp(e_{ik})}$ = 모든 값을 양수 처리 및 0 ~ 1 사이로 설정  

$c_i = \sum_{j=1}^L \alpha_{ij}h_j$  


```python
def calculate_attention(decoder_hidden, encoder_outputs, attn_projection_layer, attention_v):
    """
    어텐션 가중치를 계산하는 함수
    """
    #LSTM은 hidden_state를 [num_layer, batch_size, hidden_dim] 순으로 반환하지만
    #Attention Score 계산하려면 [batch_size, num_layer, hidden_dim] 순으로 조정. 따라서 transpose
    decoder_hidden_expanded = decoder_hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)
    
    # Badhanau 식, hidden_dim 기준으로 결합
    combined_states = torch.cat([decoder_hidden_expanded, encoder_outputs], dim=2)
    
    # 어텐션 스코어 계산
    attention_scores_prep = torch.tanh(attn_projection_layer(combined_states))
    attention_scores = attention_v(attention_scores_prep).squeeze(-1)
    
    # 어텐션 가중치 정규화
    attention_weights = F.softmax(attention_scores, dim=1)
    
    # 컨텍스트 벡터 계산
    # 가중치 합 계산하기 위해서 차원 확장
    context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
    
    return context_vector, attention_weights


```

## 7. 잘 돌아가는지 테스트


```python
# 테스트를 위한 가상 데이터 생성
batch_size = 1
src_len = 5  # 예: "<sos> she is reading a book <eos>"
hidden_dim = 8

# 임의로 decoder hidden-state와 encooder 출력 생성
test_decoder_hidden = torch.randn(1, batch_size, hidden_dim)  # [1, 1, hidden_dim]
test_encoder_outputs = torch.randn(batch_size, src_len, hidden_dim)  # [1, src_len, hidden_dim]

test_attn_projection = nn.Linear(hidden_dim * 2, hidden_dim)
test_attention_v = nn.Linear(hidden_dim, 1, bias=False)

# 테스트
test_context_vector, test_attention_weights = calculate_attention(
    test_decoder_hidden,
    test_encoder_outputs,
    test_attn_projection,
    test_attention_v
)

print("\n=== 어텐션 계산 함수 테스트 결과 ===")
print("1. 입력 데이터 형태:")
print(f"  디코더 히든 스테이트: {test_decoder_hidden.shape}")
print(f"  엔코더 출력: {test_encoder_outputs.shape}")

print("\n2. 출력 결과:")
print(f"  컨텍스트 벡터 shape: {test_context_vector.shape}")
print(f"  어텐션 가중치 shape: {test_attention_weights.shape}")

print("\n3. 어텐션 가중치 확인:")
print(f"  가중치 값:\n{test_attention_weights.squeeze()}")
print(f"  가중치 합: {test_attention_weights.sum().item():.4f}")  # 1.0이어야 함
```

    
    === 어텐션 계산 함수 테스트 결과 ===
    1. 입력 데이터 형태:
      디코더 히든 스테이트: torch.Size([1, 1, 8])
      엔코더 출력: torch.Size([1, 5, 8])
    
    2. 출력 결과:
      컨텍스트 벡터 shape: torch.Size([1, 1, 8])
      어텐션 가중치 shape: torch.Size([1, 5])
    
    3. 어텐션 가중치 확인:
      가중치 값:
    tensor([0.2270, 0.2235, 0.1708, 0.1574, 0.2213], grad_fn=<SqueezeBackward0>)
      가중치 합: 1.0000
    

## 8. 모델 가중치 초기화
- 모델의 학습 성능을 높이기 위해, 적절히 초기화


```python
def init_weights(model):
    if isinstance(model, (nn.Linear, nn.LSTM)):
        for name, param in model.named_parameters():
            # Xavier 초기화 진행. 가중치 값을 적절히 분포시킴
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            # bias 값은 전부 0으로 초기화
            elif 'bias' in name:
                nn.init.zeros_(param)

# 가중치 초기화 적용
decoder.apply(init_weights)
output_layer.apply(init_weights)
attn_projection_layer.apply(init_weights)
attention_v.apply(init_weights)
```




    Linear(in_features=8, out_features=1, bias=False)




```python
print("=== 가중치 초기화 결과 확인 ===\n")

# 1. 선형 레이어 가중치 확인
print("1. Linear Layer 확인:")
print("Output Layer:")
print(f"  Weight 범위: [{output_layer.weight.min():.4f}, {output_layer.weight.max():.4f}]")
print(f"  Bias가 0인지 확인: {torch.all(output_layer.bias == 0)}")

# 2. LSTM 레이어 확인
print("\n2. LSTM Layer 확인:")
lstm_param = next(decoder.parameters())
print(f"  Weight 범위: [{lstm_param.min():.4f}, {lstm_param.max():.4f}]")

# 3. 어텐션 레이어 확인
print("\n3. Attention Layer 확인:")
print("Projection Layer:")
print(f"  Weight 범위: [{attn_projection_layer.weight.min():.4f}, {attn_projection_layer.weight.max():.4f}]")
```

    === 가중치 초기화 결과 확인 ===
    
    1. Linear Layer 확인:
    Output Layer:
      Weight 범위: [-0.4650, 0.4626]
      Bias가 0인지 확인: True
    
    2. LSTM Layer 확인:
      Weight 범위: [-0.3500, 0.3535]
    
    3. Attention Layer 확인:
    Projection Layer:
      Weight 범위: [-0.4955, 0.4915]
    

## 9. 전체 번역 진행 및 시퀀스 출력


```python
# 생성 결과를 저장할 리스트
generated_indices = []
attention_history = []
predicted_words = []

# 디코더의 전체 타임스텝 길이 계산
target_length = tgt_tensor.size(1)

for t in range(target_length):
    # 현재 입력 임베딩
    decoder_embedded = decoder_embedding(decoder_input)
    
    # 어텐션 계산 및 컨텍스트 벡터 생성
    context_vector, attention_weights = calculate_attention(
        decoder_hidden,
        encoder_outputs,
        attn_projection_layer,
        attention_v
    )


    # 첫 번째 타임스텝에서만 shape 정보 출력
    if t == 0:
        print("\n=== 첫 번째 타임스텝의 텐서 shape 정보 ===")
        print(f"디코더 임베딩 shape: {decoder_embedded.shape}")
        print(f"컨텍스트 벡터 shape: {context_vector.shape}")
        print(f"어텐션 가중치 shape: {attention_weights.shape}")
    
    # 디코더 입력과 컨텍스트 벡터 결합
    decoder_input_combined = torch.cat([decoder_embedded, context_vector], dim=2)
    
    # 디코더 LSTM 처리
    decoder_output, (decoder_hidden, decoder_cell) = decoder(
        decoder_input_combined,
        (decoder_hidden, decoder_cell)
    )
    
    # 출력 계산
    combined_output = torch.cat([decoder_output.squeeze(1), context_vector.squeeze(1)], dim=1)
    output_hidden = torch.tanh(output_layer(combined_output))  # 비선형성 추가
    output_probs = F.softmax(output_hidden, dim=-1)
    
    # 다음 단어 예측
    predicted_idx = output_probs.argmax(dim=-1)
    generated_indices.append(predicted_idx.item())
    predicted_words.append(list(vocab_ko.keys())[predicted_idx.item()])
    
    # 어텐션 가중치 저장
    attention_history.append(attention_weights.squeeze().detach())
    
    # 다음 스텝을 위한 입력 업데이트
    decoder_input = predicted_idx.unsqueeze(0)
```

    
    === 첫 번째 타임스텝의 텐서 shape 정보 ===
    디코더 임베딩 shape: torch.Size([1, 1, 8])
    컨텍스트 벡터 shape: torch.Size([1, 1, 8])
    어텐션 가중치 shape: torch.Size([1, 7])
    


```python
# 생성 결과 출력
print("\n=== 전체 시퀀스 생성 결과 ===")
print("\n1. 입력 문장:")
print(f"  {source_sentence}")

print("\n2. 타임스텝별 생성 결과:")
for t, (word, attn) in enumerate(zip(predicted_words, attention_history)):
    print(f"\n[타임스텝 {t+1}]")
    print(f"  예측 단어: {word}")
    print("  단어별 집중도:")
    for src_word, weight in zip(['<sos>'] + source_sentence.split() + ['<eos>'], 
                              attn.numpy()):
        print(f"    {src_word:10}: {weight:.4f}")

print("\n3. 최종 생성 문장:")
print(f"  {''.join(predicted_words)}")

print("\n4. 목표 문장:")
print(f"  {target_sentence}")
```

    
    === 전체 시퀀스 생성 결과 ===
    
    1. 입력 문장:
      she is reading a book
    
    2. 타임스텝별 생성 결과:
    
    [타임스텝 1]
      예측 단어: 그녀는
      단어별 집중도:
        <sos>     : 0.1561
        she       : 0.1282
        is        : 0.1367
        reading   : 0.1247
        a         : 0.1458
        book      : 0.1529
        <eos>     : 0.1555
    
    [타임스텝 2]
      예측 단어: 있다
      단어별 집중도:
        <sos>     : 0.1561
        she       : 0.1274
        is        : 0.1383
        reading   : 0.1226
        a         : 0.1440
        book      : 0.1540
        <eos>     : 0.1575
    
    [타임스텝 3]
      예측 단어: <sos>
      단어별 집중도:
        <sos>     : 0.1566
        she       : 0.1277
        is        : 0.1383
        reading   : 0.1231
        a         : 0.1444
        book      : 0.1536
        <eos>     : 0.1564
    
    [타임스텝 4]
      예측 단어: 그들은
      단어별 집중도:
        <sos>     : 0.1564
        she       : 0.1278
        is        : 0.1383
        reading   : 0.1232
        a         : 0.1442
        book      : 0.1535
        <eos>     : 0.1565
    
    [타임스텝 5]
      예측 단어: 그들은
      단어별 집중도:
        <sos>     : 0.1557
        she       : 0.1272
        is        : 0.1374
        reading   : 0.1227
        a         : 0.1437
        book      : 0.1550
        <eos>     : 0.1583
    
    [타임스텝 6]
      예측 단어: 그는
      단어별 집중도:
        <sos>     : 0.1567
        she       : 0.1284
        is        : 0.1393
        reading   : 0.1234
        a         : 0.1439
        book      : 0.1532
        <eos>     : 0.1551
    
    3. 최종 생성 문장:
      그녀는있다<sos>그들은그들은그는
    
    4. 목표 문장:
      그녀는 책을 읽고 있다
    

모델 초기화할 때, 당연히 단어 빈도순, 중요도 순으로 초기화해야하는데 랜덤이 아닌 적당한 값으로 초기화하다 보니 애매한 결과가 나옴  
다음엔 train, test 과정까지 넣을 예정
