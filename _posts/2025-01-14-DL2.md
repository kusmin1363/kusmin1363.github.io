---
layout: single
title:  "DL-2. 딥러닝 모델 만들기 2"
---

# 딥러닝 모델 만들기 2

## 1. torch.nn.functional
- functional은 NN의 기본 구성 요소를 함수로 사용할 수 있도록 제공됨.
- torch.nn과 다르게 가중치를 직접 지정해줘야 함. 
- 커스텀 연산을 하거나, 직접적으로 연산에 대해 제어해주고 싶을 때 사용
- parameter를 따로 생성하지 않기 때문에, 임시로 사용할 때도 자주 사용.


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```


```python
input_data = torch.rand(1, 3)

# 가중치와 바이어스를 직접 정의
weight = torch.rand(2, 3) # 출력 차원이 2, 입력 차원이 3인 가중치
bias = torch.rand(2)  # 출력 차원에 해당하는 바이어스

# F.linear 함수를 사용하여 연산을 수행
output = F.linear(input_data, weight, bias)

print('\nW: \n', weight)
print('x: \n', input_data)
print('\nb: \n', bias)
print('\noutput: \n', output)
```

    
    W: 
     tensor([[0.5765, 0.8839, 0.1479],
            [0.7521, 0.4693, 0.3105]])
    x: 
     tensor([[0.4414, 0.7011, 0.6663]])
    
    b: 
     tensor([0.7847, 0.8173])
    
    output: 
     tensor([[1.7575, 1.6852]])
    

### functional의 메서드는 nn의 메서드와 살짝 다르니 참고
- nn과 다르게 메서드 이름에 대문자가 없음
- 대부분의 함수는 동일
- weight, bias를 안 넣어주면 작동 안함


```python
input_data = torch.rand(1, 1, 5, 5)

# 컨볼루션 커널(가중치) 정의: 출력 채널 1, 입력 채널 1, 커널 크기 3x3
weight = torch.rand(1, 1, 3, 3)
# 선택적으로 바이어스 정의: 출력 채널 1에 대한 바이어스
bias = torch.rand(1)

# F.conv2d 함수를 사용하여 2D 컨볼루션 수행
output = F.conv2d(input_data, weight, bias, stride=1, padding=1)
print(output)
```


```python
input_tensor = torch.randn(3, 3)

# 모델을 학습할 때는 training=True로 적용
output1 = F.dropout(input_tensor, p=0.5, training=True)

# 모델을 학습하지 않을 때는 training=False로 적용
output2 = F.dropout(input_tensor, p=0.5, training=False)
```

## 2. Functional로 모델만들어 보기


```python
class SimpleNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):
        super(SimpleNeuralNetwork, self).__init__()
        # 가중치와 바이어스를 nn.Parameter로 직접 정의, nn.Parameter가 변수들을 자동으로 관리해줌 
        # torch.randn함수를 이용해 초기값을 랜덤으로 지정
        self.weight1 = nn.Parameter(torch.randn(hidden_size, input_size))
        self.bias1 = nn.Parameter(torch.randn(hidden_size))
        
        # 첫번째 Layer와 동일하게 진행행
        self.weight2 = nn.Parameter(torch.randn(output_size, hidden_size))
        self.bias2 = nn.Parameter(torch.randn(output_size))
        
        # 드롭아웃 비율
        self.dropout_rate = dropout_rate

    def forward(self, x):
        # 첫 번째 완전 연결층을 통과한 후 ReLU 활성화 함수를 적용
        x = F.relu(F.linear(x, self.weight1, self.bias1))
        # 드롭아웃 적용
        x = F.dropout(x, p=self.dropout_rate, training=self.training)
        # 두 번째 완전 연결층을 통과
        x = F.linear(x, self.weight2, self.bias2)
        return x

# 모델 인스턴스를 생성하고, 드롭아웃 비율은 0.5로 설정
model = SimpleNeuralNetwork(input_size=10, hidden_size=5, output_size=2, dropout_rate=0.5)
print(model)
```

    SimpleNeuralNetwork()
    

## 3. 사용자 정의 연산 만들기
- pytorch 모델에 없는 연구에 필요한 자신만의 연산을 만들 때 사용
- 사용자 정의 함수 만드는거랑 비슷한 맥락

$$ y = Wx + b $$


```python
def custom_layer(x, weight, bias): 
    x = torch.matmul(weight, x) + bias
    return x

input_tensor = torch.randn(10, 1)

# 가중치와 바이어스 초기화
weight = torch.randn(2, 10)
bias = torch.randn(2)

# 함수형 레이어 사용
output = custom_layer(input_tensor, weight, bias)
print(output)
```

    tensor([[ 1.0337, -1.2588],
            [-1.2407, -3.5332]])
    

$$ y =  sin(xW^T)+b $$


```python
def custom_sine_layer(x, weight, bias):
    w = weight.t()
    x = torch.sin(torch.matmul(x,w))+bias
    return x

# 예제 입력 데이터
input_tensor = torch.randn(1, 10)
# 가중치와 바이어스 초기화
weight = torch.randn(2, 10)
bias = torch.randn(1)

# 함수형 레이어 사용
output = custom_sine_layer(input_tensor, weight, bias)
output
```




    tensor([[-1.4300,  0.0864]])



### 클래스로 설정


```python
import torch.nn as nn

class CustomLinearLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(CustomLinearLayer, self).__init__()
        # 가중치의 차원을 [output_dim, input_dim]으로 수정
        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))
        self.bias = nn.Parameter(torch.randn(output_dim))
        
    def forward(self, x):
        y = torch.matmul(x,self.weight.t())+self.bias
        return y
    
class CustomMLP(nn.Module):
    def __init__(self):
        super(CustomMLP, self).__init__()
        self.fc1 = CustomLinearLayer(20, 50)
        self.fc2 = CustomLinearLayer(50, 30)
        self.fc3 = CustomLinearLayer(30, 3)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```


```python
# 모델 초기화
model = CustomMLP()

# 배치 크기 5, 입력 크기 20인 임의의 입력 데이터
x = torch.randn(5, 20)

# 모델을 통과하여 예측 수행
output = model(x)

print("모델의 예측 출력:\n", output)
print("출력 크기:", output.size())
```

    모델의 예측 출력:
     tensor([[ -17.4940,  -36.9009,  -69.5740],
            [  55.1812,  -25.7869,  -53.6616],
            [  72.5501,  -10.1371,  -95.5875],
            [  91.3765, -101.4580, -120.8993],
            [ 137.0911,  -31.6071,  -40.0077]], grad_fn=<AddBackward0>)
    출력 크기: torch.Size([5, 3])
    

## 4. 손실 함수
- 딥러닝 모델을 학습시키는 기준
- MSE, Cross-Entropy Loss, Binary Cross-Entropy Loss 등 다양한 Loss 존재
- 손실함수 계산해주는 인스턴스만 생성해주면 끝  
![Loss](/assets/images/20250114.loss1.png)


```python
import torch.nn as nn

criterion = nn.CrossEntropyLoss()
print(criterion)

criterion2 = nn.MSELoss()
print(criterion2)
```

    CrossEntropyLoss()
    MSELoss()
    

## 5. Optimizer
- weight를 어떻게 조정해야 모델의 Loss를 최소화할 수 있을까? => Optimizer
- Loss가 최소화 되는 방향으로 weight 변화 = gradient descent 방법(기본 Optimizer)
- 이외에도 Stochastic Gradient Descent, Adam 등 다양한 Optimizer 존재, 상황에 맞는 걸로 사용  

![Optimizer](/assets/images/20250114.optim1.png)

- model.parameter, lr(learning rate)을 전달해줘야 함.


```python
import torch.optim as optim

# 딥러닝 모델 정의
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(in_features=1, out_features=5)  # 첫 번째 선형 레이어
        self.relu = nn.ReLU()  # ReLU 활성화 함수
        self.layer2 = nn.Linear(in_features=5, out_features=1)  # 두 번째 선형 레이어

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

# 모델 인스턴스 생성
model = SimpleNN()

# SGD 옵티마이저 설정
optimizer = optim.SGD(model.parameters(), lr=0.01)
print(optimizer)
```

    SGD (
    Parameter Group 0
        dampening: 0
        differentiable: False
        foreach: None
        fused: None
        lr: 0.01
        maximize: False
        momentum: 0
        nesterov: False
        weight_decay: 0
    )
    

### 모델의 parameter 직접 보는 법


```python
for name, param in model.named_parameters():
    print(f"Parameter name: {name}")
    print(param)
    print()

```

    Parameter name: layer1.weight
    Parameter containing:
    tensor([[ 0.6976],
            [ 0.3385],
            [-0.1405],
            [-0.5576],
            [-0.4135]], requires_grad=True)
    
    Parameter name: layer1.bias
    Parameter containing:
    tensor([ 0.6902, -0.2664, -0.5748,  0.9353,  0.7018], requires_grad=True)
    
    Parameter name: layer2.weight
    Parameter containing:
    tensor([[0.1667, 0.3086, 0.4413, 0.2174, 0.2699]], requires_grad=True)
    
    Parameter name: layer2.bias
    Parameter containing:
    tensor([-0.3684], requires_grad=True)
    
    
