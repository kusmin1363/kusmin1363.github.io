---
layout: single
title:  "DL-12. Attention(이론)"
---

# Attention Mechanism(이론)

## 1. Attention Mechanism 등장 이유
- Seq2Seq의 한계로 인해서 Attention 등장  
     1. Bottleneck 현상 : Encoder의 마지막 Hidden-state가 컨텍스트 벡터로서 Decoder의 입력으로 들어감  
        - 컨텍스트 벡터는 고정된 크기를 가지고 있으므로, 긴 문장을 요약해 처리하다보면 일부 정보가 누락될 수 있음  
     2. 긴 문장 처리의 어려움 : 1번과 비슷한 이유  

## 2. Attention Mechanism
![png](/assets/images/DL-12-image.png)


- **헷갈리지 말것**
: 즉, 은닉 상태는 개별 단어의 처리 결과이고, 컨텍스트 벡터는 그 은닉 상태들을 상황에 맞게 재조합하여 현재 디코더가 집중해야 할 문맥 정보를 제공하는 역할

- Attention Mehanism과 Seq2Seq랑 다른 점
  1. 디코더가 각 시점마다 동적으로 인코더 전체의 hidden-state들로부터 컨텍스트 벡터를 재계산  
         - 즉, Seq2Seq처럼 hidden-state랑 입력만 보고 해석하는게 아니라, 원본 단어도 참가하면서 진행  
         - 이전 시점의 단어, 현재 은닉 상태, 컨텍스트 벡터로 다음 단어를 예측  
  2. Attention Score 및 Weight의 도입  
         - 디코더가 각 시점에서 인코더의 은닉 상태(또는 Key)와 얼마나 "잘 맞는지" 혹은 "관련 있는지"를 수치로 나타내는 값  
         - 예측할 단어를 원래 시퀀스의 단어 중 어떤 걸 보고 예측할지 결정 


## 3. Attention Mechanism 구조
- Attention Score를 계산해 입력 단어들의과의 연관성을 계산 -> Attention Score를 바탕으로 Attention Weight를 생성 -> Weight를 사용해 Context Vector 생성 -> 최종 계산 및 단어 출력

- Attention Mechanism : Encoder, Attention Layer, Decoder의 구조를 가짐 ( **Seq2Seq** 개념에서 Attention Layer 추가 )
    1.  Encoder : 입력 문장의 각 단어를 처리해서, 각각의 LSTM cell에 hidden-state 생성. 모든 hidden-state를 Attention Layer에 전달
    2.  Attention Layer : Attention Score, Attention Weight, Context Vector 생성  
             - 즉, 디코더가 매 시점 적절한 단어를 예측하는 걸 보조해주는 역할임
    3.  Decoder : Context Vector, Decoder 내부 hidden-state, 단어 입력을 바탕으로 다음 단어 예측

## 4. Attention Score

- i 시점에서 디코더의 hidden-state : $s_i$, 그럼 이전 시점에서의 hidden-state : $s_{i-1}$
- 특정 시점에서의 인코더의 hidden-state : $h_j$  
- Attention Score 계산해주는 함수 : $a$
- 디코더의 i 시점을 기준으로, 특정 단어를 보고 계산한 Attention Score : $e_{ij} = a(s_{i-1},h_j)$

Q = Query : t 시점의 디코더 셀에서의 은닉 상태  
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들 - 둘 다 비슷한 개념이지만, 용도가 조금 다름.
V = Values : 모든 시점의 인코더 셀의 은닉 상태들  


다양한 attention 함수 존재  
  1. Dot Product Attention : $e_{ij} = s_{i-1}^\top h_j$ = 단순하게 두 벡터가 얼마나 유사한지 내적으로 계산   
  2. Scaled Dot Product Attention : $e_{ij} = \frac{s_{i-1}^\top h_j}{\sqrt{d_k}}$ = 1번에서 너무 내적값이 커지는 걸 막기 위해, Key 벡터의 차원 크기에 비례해 나눠줌   
  3. Luong Attention : $e_{ij} = (W_s s_{i-1})^\top h_j$ = 1에서 은닉 상태와 학습 가능한 가중치 행렬(W)를 곱해 새롭게 공간을 변환하고 내적 수행    
  4. Bahdanau Attention(Additive Attention) : $e_{ij} = v_a^\top tanh(W_a[s_{i-1}; h_j])$ = 신경망을 통해 비선형적으로 결합  
         - $[s_{i-1};h_j] $ = 두개를 직렬 연결  
         - 연결된 벡터를 $W_a$를 통해 변환 후, tanh로 비선형 결합, 마지막으로 $v_a$를 통해 최종 스코어 계산
  - 이외에도 제일 중요한 Self-Attention이 있지만, Transformer 할 때 다룰 예정

## 5. Attention Weight & Context Vector
- $e_{ij}$는 raw한 값. 정규화 처리 및 실사용을 위해 Softmax 연산 진행  
$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^L exp(e_{ik})}$ = 모든 값을 양수 처리 및 0 ~ 1 사이로 설정

- Attention Weight를 이용해 Context Vector 계산  
$c_i = \sum_{j=1}^L \alpha_{ij}h_j$  
$\alpha_{ij}$ : 디코더의 현재 단계와 입력 문장의 각 단어 간의 Attention Weight  
$h_j$ : 인코더의 각 단어에 대한 은닉 상태  
가중치가 높으면 은닉 상태 많이 반영, 적으면 적게 반영  

## 6. 최종 단어 예측

![png](/assets/images/DL-12-image-2.png)

- FC layer를 지나 Vocabulary 내의 단어들에 대한 점수로 변환되고, Softmax를 통해서 결국 Vocabulary 내의 단어로 변환됨

이론만 우선 다루고, 구현은 다음 포스트에 다루겠다.


