---
layout: single
title:  "DL-13. Attention(구현 2)"
---

# Attention Mechanism(구현 2)

## 1. Vocabulary 정의 및 인덱스 변환


```python
import torch 

# 어휘 사전 정의
# 영어 어휘 사전
vocab_en = {
    "<sos>": 0, "<eos>": 1,  
    "i": 2, "am": 3, "he": 4, "she": 5, "is": 6, 
    "you": 7, "are": 8, "we": 9, "they": 10,
    "book": 11, "books": 12, "reading": 13, "writing": 14, "eating": 15,
    "a": 16, "the": 17, "in": 18, "on": 19, "at": 20,
    "home": 21, "school": 22, "library": 23
}

# 한국어 어휘 사전
vocab_ko = {
    "<sos>": 0, "<eos>": 1,  
    "나는": 2, "너는": 3, "그는": 4, "그녀는": 5, "우리는": 6, "그들은": 7,
    "책을": 8, "읽고": 9, "쓰고": 10, "먹고": 11,
    "있다": 12, "있습니다": 13,
    "집에서": 14, "학교에서": 15, "도서관에서": 16
}

# 학습용 예제 문장 쌍
train_pairs = [
    ("she is reading a book", "그녀는 책을 읽고 있다"),
    ("i am reading a book", "나는 책을 읽고 있다"),
    ("they are reading books", "그들은 책을 읽고 있다"),
    ("he is writing at school", "그는 학교에서 쓰고 있다"),
    ("we are eating at home", "우리는 집에서 먹고 있다")
]

def sentence_to_indices(sentence: str, vocab: dict) -> torch.Tensor:
    """문장을 인덱스 시퀀스로 변환 (시작과 종료 토큰 포함)"""
    words = sentence.lower().split()
    indices = [vocab["<sos>"]] + [vocab[word] for word in words] + [vocab["<eos>"]] 
    return torch.tensor(indices).unsqueeze(0)
```


```python

print("=== 데이터셋 구성 결과 ===")
print(f"영어 어휘 크기: {len(vocab_en)}")
print(f"한국어 어휘 크기: {len(vocab_ko)}") 

print("\n=== 입력 문장 처리 결과 ===")
for src_sentence, tgt_sentence in train_pairs[:2]:
    src_tensor = sentence_to_indices(src_sentence, vocab_en)
    tgt_tensor = sentence_to_indices(tgt_sentence, vocab_ko)
    
    print(f"\n[원문]")
    print(f"EN: {src_sentence}")
    print(f"KO: {tgt_sentence}")
    print(f"\n[텐서 변환 결과]")
    print(f"입력 텐서 shape: {src_tensor.shape}")
    print(f"출력 텐서 shape: {tgt_tensor.shape}")
    print(f"입력 인덱스: {src_tensor.squeeze().tolist()}")
    print(f"출력 인덱스: {tgt_tensor.squeeze().tolist()}")
    print("-" * 50)
```

    === 데이터셋 구성 결과 ===
    영어 어휘 크기: 24
    한국어 어휘 크기: 17
    
    === 입력 문장 처리 결과 ===
    
    [원문]
    EN: she is reading a book
    KO: 그녀는 책을 읽고 있다
    
    [텐서 변환 결과]
    입력 텐서 shape: torch.Size([1, 7])
    출력 텐서 shape: torch.Size([1, 6])
    입력 인덱스: [0, 5, 6, 13, 16, 11, 1]
    출력 인덱스: [0, 5, 8, 9, 12, 1]
    --------------------------------------------------
    
    [원문]
    EN: i am reading a book
    KO: 나는 책을 읽고 있다
    
    [텐서 변환 결과]
    입력 텐서 shape: torch.Size([1, 7])
    출력 텐서 shape: torch.Size([1, 6])
    입력 인덱스: [0, 2, 3, 13, 16, 11, 1]
    출력 인덱스: [0, 2, 8, 9, 12, 1]
    --------------------------------------------------
    

## 2. 모델 Layer 정의(임베딩, 인코더, 디코더, Attention Layer)
Attention Layer : Badhanau  
 $e_{ij} = v_a^\top tanh(W_a[s_{i-1}; h_j])$


```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 재현성을 위한 시드 설정
torch.manual_seed(42)

# 모델 파라미터 설정
embedding_dim = 8    # 임베딩 차원
hidden_dim = 8       # LSTM 히든 차원

# 임베딩 레이어 정의
encoder_embedding = torch.nn.Embedding(len(vocab_en), embedding_dim)
decoder_embedding = torch.nn.Embedding(len(vocab_ko), embedding_dim)

# 인코더/디코더 정의
encoder = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
decoder = torch.nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)

# 어텐션 레이어 정의
attn_projection_layer = torch.nn.Linear(hidden_dim+embedding_dim, hidden_dim)
attention_v = torch.nn.Linear(hidden_dim, 1, bias=False)

# 출력 레이어 정의
output_layer = torch.nn.Linear(hidden_dim*2,len(vocab_ko))
```


```python
# 결과 확인을 위한 코드
print("\n=== 모델 아키텍처 구성 ===")
print("\n[임베딩 레이어]")
print(f"인코더 임베딩: 입력 크기={len(vocab_en)}, 출력 크기={embedding_dim}")
print(f"디코더 임베딩: 입력 크기={len(vocab_ko)}, 출력 크기={embedding_dim}")

print("\n[LSTM 레이어]")
print(f"인코더 LSTM: 입력 크기={embedding_dim}, 은닉 크기={hidden_dim}")
print(f"디코더 LSTM: 입력 크기={embedding_dim + hidden_dim}, 은닉 크기={hidden_dim}")

print("\n[어텐션 레이어]")
print(f"투영 레이어: 입력 크기={hidden_dim * 2}, 출력 크기={hidden_dim}")
print(f"어텐션 가중치: 입력 크기={hidden_dim}, 출력 크기=1")

print("\n[출력 레이어]")
print(f"출력 레이어: 입력 크기={hidden_dim * 2}, 출력 크기={len(vocab_ko)}")

```

    
    === 모델 아키텍처 구성 ===
    
    [임베딩 레이어]
    인코더 임베딩: 입력 크기=24, 출력 크기=8
    디코더 임베딩: 입력 크기=17, 출력 크기=8
    
    [LSTM 레이어]
    인코더 LSTM: 입력 크기=8, 은닉 크기=8
    디코더 LSTM: 입력 크기=16, 은닉 크기=8
    
    [어텐션 레이어]
    투영 레이어: 입력 크기=16, 출력 크기=8
    어텐션 가중치: 입력 크기=8, 출력 크기=1
    
    [출력 레이어]
    출력 레이어: 입력 크기=16, 출력 크기=17
    

## 3. Attention Score, Weight, Context Vector 계산
 $e_{ij} = v_a^\top tanh(W_a[s_{i-1}; h_j])$ 에 따라서 tanh 처리하고, $v_a$와도 연산    
 연산 값은 logit 값(raw 값)이기 때문에 음수도 가능, Softmax까지 해줘야 0~1 사이 값 나옴  

 $\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^L exp(e_{ik})}$ = 모든 값을 양수 처리 및 0 ~ 1 사이로 설정  

$c_i = \sum_{j=1}^L \alpha_{ij}h_j$  


```python
def calculate_attention(decoder_hidden, encoder_outputs, attn_projection_layer, attention_v):
    #LSTM은 hidden_state를 [num_layer, batch_size, hidden_dim] 순으로 반환하지만
    #Attention Score 계산하려면 [batch_size, num_layer, hidden_dim] 순으로 조정. 따라서 transpose
    #repeat은 인코더 출력 길이에 맞게 확장하기 위함
    decoder_hidden_expanded = decoder_hidden.transpose(0, 1).repeat(1, encoder_outputs.size(1), 1)
    
    # Badhanau 식: hidden_dim 기준으로 결합
    combined_states = torch.cat([decoder_hidden_expanded, encoder_outputs], dim=2)
    
    # 어텐션 스코어 계산
    attention_scores_prep = torch.tanh(attn_projection_layer(combined_states))
    attention_scores = attention_v(attention_scores_prep).squeeze(-1)
    
    # 어텐션 가중치 정규화
    attention_weights = F.softmax(attention_scores, dim=1)
    
    # 컨텍스트 벡터 계산
    # 가중치 합 계산하기 위해서 차원 확장
    context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
    
    return context_vector, attention_weights
```

## 4. 학습 파라미터 설정


```python
learning_rate = 0.01 # 학습률

# 학습 가능한 파라미터 설정
model_parameters = [
    encoder.parameters(),          # 인코더 LSTM
    decoder.parameters(),          # 디코더 LSTM
    encoder_embedding.parameters(),  # 인코더 임베딩
    decoder_embedding.parameters(),  # 디코더 임베딩
    attn_projection_layer.parameters(),  # 어텐션 변환
    attention_v.parameters(),         # 어텐션 가중치
    output_layer.parameters()         # 출력 레이어
]

# 모든 파라미터를 하나의 리스트로 결합
parameters = []
for param_group in model_parameters:
    parameters.extend(list(param_group))

# 옵티마이저와 손실 함수 설정
optimizer = torch.optim.Adam(parameters, lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()


print("\n=== 학습 설정 ===")
print(f"학습률: {optimizer.param_groups[0]['lr']}")
print(f"옵티마이저: {optimizer.__class__.__name__}")
print(f"손실 함수: {criterion.__class__.__name__}")
```

    
    === 학습 설정 ===
    학습률: 0.01
    옵티마이저: Adam
    손실 함수: CrossEntropyLoss
    

## 5. 단일 문장 학습되는지 확인


```python
# 데이터 준비
ex_src_tensor = sentence_to_indices(train_pairs[0][0], vocab_en)
ex_tgt_tensor = sentence_to_indices(train_pairs[0][1], vocab_ko)


# Forward Pass
optimizer.zero_grad()

# 인코더
ex_src_embedded = encoder_embedding(ex_src_tensor)
ex_enc_outputs, (ex_enc_hidden, ex_enc_cell) = encoder(ex_src_embedded)

# 디코더 (첫 스텝)
ex_dec_input = torch.tensor([[vocab_ko["<sos>"]]])
ex_dec_hidden = ex_enc_hidden
ex_dec_embedded = decoder_embedding(ex_dec_input)

# Attention
ex_context_vec, ex_attn_weights = calculate_attention(
    ex_dec_hidden,
    ex_enc_outputs,
    attn_projection_layer,
    attention_v
)

# 디코더 출력
ex_dec_combined = torch.cat([ex_dec_embedded, ex_context_vec], dim=2)
ex_dec_output, (ex_dec_hidden, ex_dec_cell) = decoder(
    ex_dec_combined, 
    (ex_dec_hidden, ex_enc_cell)
)
ex_output_vec = torch.cat([ex_dec_output.squeeze(1), ex_context_vec.squeeze(1)], dim=1)
ex_output = output_layer(ex_output_vec)

# 손실 계산 및 역전파
print("\n[손실 계산 과정의 텐서 값]")
print(f"목표 텐서(ex_tgt_tensor[0, 1]) shape: {ex_tgt_tensor[0, 1].shape}")  # [1]
ex_target = ex_tgt_tensor[0, 1].unsqueeze(0)

print(f"예측 텐서(ex_output) shape: {ex_output.shape}")  # [1, 1, vocab_ko_size]
print(f"목표 텐서(ex_target) shape: {ex_target.shape}")  # [1]

loss_ex = criterion(ex_output, ex_target)
loss_ex.backward()

# 옵티마이저 스텝
optimizer.step()

print("\n[파라미터 업데이트 후 상태]")
print(f"인코더 첫 번째 레이어 평균: {encoder.weight_ih_l0.data.mean().item():.6f}")
print(f"디코더 첫 번째 레이어 평균: {decoder.weight_ih_l0.data.mean().item():.6f}")

print("\n[학습 결과]")
print(f"입력 문장: {train_pairs[0][0]}")
print(f"목표 문장: {train_pairs[0][1]}")
print(f"손실: {loss_ex.item():.4f}")

print("한 스텝의 학습 완료")
```

    
    [학습 전 파라미터 상태]
    인코더 첫 번째 레이어 평균: -0.003554
    디코더 첫 번째 레이어 평균: -0.002276
    
    [손실 계산 과정의 텐서 값]
    목표 텐서(ex_tgt_tensor[0, 1]) shape: torch.Size([])
    예측 텐서(ex_output) shape: torch.Size([1, 17])
    목표 텐서(ex_target) shape: torch.Size([1])
    
    [파라미터 업데이트 후 상태]
    인코더 첫 번째 레이어 평균: -0.003801
    디코더 첫 번째 레이어 평균: -0.001961
    
    [학습 결과]
    입력 문장: she is reading a book
    목표 문장: 그녀는 책을 읽고 있다
    손실: 2.6433
    한 스텝의 학습 완료
    

## 6. 모델 초기화 및 학습 함수


```python
# 모델 초기화
def init_weights(model):
    if isinstance(model, (nn.Linear, nn.LSTM)):
        for name, param in model.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
                
def train_step(src_tensor, tgt_tensor, optimizer, criterion):
    # 기울기 초기화
    optimizer.zero_grad()
    
    # 인코더
    src_embedded = encoder_embedding(src_tensor)
    enc_outputs, (enc_hidden, enc_cell) = encoder(src_embedded)
    
    # 디코더 초기 상태
    dec_input = torch.tensor([[vocab_ko["<sos>"]]])
    dec_hidden = enc_hidden
    
    # 손실 초기화
    batch_loss = 0
    
    # Teacher forcing을 사용한 디코더 학습
    for t in range(1, tgt_tensor.size(1)):
        # 디코더 입력 임베딩
        dec_embedded = decoder_embedding(dec_input)
        
        # Attention 계산
        context_vector, attention_weights = calculate_attention(
            dec_hidden,
            enc_outputs,
            attn_projection_layer,
            attention_v
        )
        
        # 디코더 스텝
        dec_input_combined = torch.cat([dec_embedded, context_vector], dim=2)
        dec_output, (dec_hidden, dec_cell) = decoder(
            dec_input_combined,
            (dec_hidden, enc_cell)
        )
        
        # 출력 계산
        output_vector = torch.cat([dec_output.squeeze(1), context_vector.squeeze(1)], dim=1)
        output = output_layer(output_vector)
        
        # 손실 계산
        loss = criterion(output, tgt_tensor[:, t])
        batch_loss += loss
        
        # 다음 스텝 입력 (Teacher forcing)
        dec_input = tgt_tensor[:, t].unsqueeze(1)
    
    # 평균 손실 계산
    batch_loss = batch_loss / (tgt_tensor.size(1) - 1)
    
    # 역전파 및 옵티마이저 스텝
    batch_loss.backward()
    torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)
    optimizer.step()
    
    return batch_loss.item()
```

## 7. 학습


```python
# 모델 가중치 초기화
init_weights(encoder)
init_weights(decoder)
init_weights(attn_projection_layer)
init_weights(output_layer)


# 전체 학습 시작
n_epochs = 200
print("\n=== 전체 데이터셋 학습 시작 ===")
print(f"전체 에폭: {n_epochs}")
print(f"학습 데이터 크기: {len(train_pairs)}")

loss_history = []

for epoch in range(n_epochs):
    epoch_loss = 0
    
    for src_sentence, tgt_sentence in train_pairs:
        # 데이터 준비
        src_tensor = sentence_to_indices(src_sentence, vocab_en)
        tgt_tensor = sentence_to_indices(tgt_sentence, vocab_ko)
        
        # 학습 스텝 수행
        step_loss = train_step(src_tensor, tgt_tensor, optimizer, criterion)
        epoch_loss += step_loss
    
    # 에폭 평균 손실 계산
    avg_epoch_loss = epoch_loss / len(train_pairs)
    loss_history.append(avg_epoch_loss)
    
    # 진행 상황 출력 (10 에폭마다)
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}/{n_epochs} : avg loss: {avg_epoch_loss:.4f} ")

print("\n학습 완료!")
print(f"최종 손실: {loss_history[-1]:.4f}")
```

    
    === 전체 데이터셋 학습 시작 ===
    전체 에폭: 200
    학습 데이터 크기: 5
    Epoch 20/200 : avg loss: 0.2362 
    Epoch 40/200 : avg loss: 0.0472 
    Epoch 60/200 : avg loss: 0.0216 
    Epoch 80/200 : avg loss: 0.0129 
    Epoch 100/200 : avg loss: 0.0087 
    Epoch 120/200 : avg loss: 0.0063 
    Epoch 140/200 : avg loss: 0.0048 
    Epoch 160/200 : avg loss: 0.0038 
    Epoch 180/200 : avg loss: 0.0031 
    Epoch 200/200 : avg loss: 0.0025 
    
    학습 완료!
    최종 손실: 0.0025
    

## 8. 학습 그래프 그리기


```python
import matplotlib.pyplot as plt

# 학습 곡선 그리기
plt.figure(figsize=(10, 5))
plt.plot(range(1, n_epochs + 1), loss_history, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Time')
plt.grid(True)
plt.legend()
plt.yscale('log')  # y축을 로그 스케일로 표시
plt.show()

print("\n=== Step 6: 학습 곡선 분석 ===")
print(f"초기 손실: {loss_history[0]:.4f}")
print(f"최종 손실: {loss_history[-1]:.4f}")
print(f"손실 감소율: {((loss_history[0] - loss_history[-1]) / loss_history[0] * 100):.2f}%")
```


![png](/assets/images/2025-02-15-DL14_19_0.png)
    


    
    === Step 6: 학습 곡선 분석 ===
    초기 손실: 2.8022
    최종 손실: 0.0025
    손실 감소율: 99.91%
    

## 9. 새로운 데이터에 대한 성능 테스트


```python
def translate(sentence, max_length=20):
    # 모델을 평가 모드로 설정
    encoder.eval()
    decoder.eval()
    
    with torch.no_grad():  # 추론 시에는 gradient 계산 불필요
        # 입력 문장 텐서 변환
        src_tensor = sentence_to_indices(sentence, vocab_en)
        
        # 인코더 forward pass
        src_embedded = encoder_embedding(src_tensor)
        encoder_outputs, (encoder_hidden, encoder_cell) = encoder(src_embedded)
        
        # 디코더 초기 입력 (<sos> 토큰)
        decoder_input = torch.tensor([[vocab_ko["<sos>"]]])
        decoder_hidden = encoder_hidden
        
        # 번역 결과를 저장할 리스트
        translated_tokens = []
        attention_weights_history = []
        
        # 디코더를 통한 번역 생성
        for t in range(max_length):
            # 1. 현재 입력 임베딩
            decoder_embedded = decoder_embedding(decoder_input)
            
            # 2. Attention 계산
            context_vector, attention_weights = calculate_attention(
                decoder_hidden,
                encoder_outputs,
                attn_projection_layer,
                attention_v
            )
            
            # attention 가중치 저장 (numpy 배열로 변환)
            attention_weights_history.append(attention_weights.cpu().numpy()[0])
            
            # 3. 디코더 LSTM 스텝
            decoder_input_combined = torch.cat([decoder_embedded, context_vector], dim=2)
            decoder_output, (decoder_hidden, decoder_cell) = decoder(decoder_input_combined, 
                                                                (decoder_hidden, encoder_cell))
            
            # 4. 출력 레이어
            output_vector = torch.cat([decoder_output.squeeze(1), context_vector.squeeze(1)], dim=1)
            output_hidden = torch.tanh(output_layer(output_vector))  # 비선형성 추가
            output_probs = F.softmax(output_hidden, dim=-1)
         
            # 5. 다음 단어 예측
            predicted_token = output_probs.argmax(dim=-1).item()
            # 예측된 토큰 저장 (이미 같은 시퀀스가 나오면 중단)
            if len(translated_tokens) >= 3 and \
                translated_tokens[-3:] == translated_tokens[-6:-3]:  # 반복 패턴 감지
                translated_tokens = translated_tokens[:-3]  # 반복된 부분 제거
                break
                
            translated_tokens.append(predicted_token)
            
            # <eos> 토큰이 나오면 번역 종료
            if predicted_token == vocab_ko["<eos>"]:
                break
            
            # 다음 스텝을 위한 입력 업데이트
            decoder_input = torch.tensor([[predicted_token]])
        
        # 토큰을 단어로 변환
        idx_to_ko = {v: k for k, v in vocab_ko.items()}
        translated_words = [idx_to_ko[token] for token in translated_tokens]
        
        # <eos> 토큰이 있으면 제거
        if vocab_ko["<eos>"] in translated_tokens:
            translated_words = translated_words[:-1]
        
        return translated_words, attention_weights_history
```


```python
train_test_sentences = [
    "she is reading a book",
    "i am reading a book",
    "he is writing at school"
]

for sentence in train_test_sentences:
    translated_words, _ = translate(sentence)
    print(f"\n입력: {sentence}")
    print(f"번역: {' '.join(translated_words)}")

```


```python
new_test_sentences = [
    "he is reading at library",  # 새로운 장소
    "they are writing books",    # 새로운 조합
    "i am eating at school"      # 새로운 조합
]

for sentence in new_test_sentences:
    translated_words, _ = translate(sentence)
    print(f"\n입력: {sentence}")
    print(f"번역: {' '.join(translated_words)}")
```

    
    입력: he is reading at library
    번역: 그는 학교에서 쓰고 있다
    
    입력: they are writing books
    번역: 그들은 책을 읽고 있다
    
    입력: i am eating at school
    번역: 나는 책을 읽고 있다
    

새로운 데이터에 대해 시험했을 때, 번역 결과는 좋지 못함  
원인 :  
1. 학습 데이터 부족
2. Vocab 의 제한  
3. 패턴 인식으로 인한 일반화 능력 부족
