---
layout: single
title:  "DL-3. 딥러닝 모델 만들기 3 - 실습"
---

# 실습 진행

## 1. 실제 딥러닝 모델 학습 과정
- 딥러닝 모델은 데이터 전처리, 모델 정의, 손실 함수, Optimizer 선택이 완료된 이후에 진행

딥러닝 모델 학습 순서
1. 모델을 학습 모드로 설정
2. 데이터 배치 로드 - DataLoader를 통해 batch 단위로 데이터 불러옴. 모델의 입력으로 batch 사용
3. 기울기 초기화 - zero_grad()를 통해 이전 스텝에서 계산했던 기울기 초기화
4. 순전파(Forward_pass) - 모델 연산 진행
5. Loss 계산 - 실제 답과 비교해 Loss 계산
6. 역전파(Backward_pass) - loss.backward()를 통해 Loss에 대한 기울기를 계산. 기울기를 바탕으로 Model.parameter의 기울기 계산
7. Parameter Update - parameter 기울기 계산 바탕으로 새롭게 parameter 변경

### 진짜 중요한 예시
- 어떻게 데이터 셋을 생성하는가


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 더미 정형 데이터와 레이블 생성
num_samples = 1000  # 데이터 샘플의 수
num_test_samples = 200  # 테스트 데이터 샘플의 수
num_features = 10   # 입력 특성의 수


# 1. 랜덤 학습/테스트 데이터셋 생성
features = torch.randn(num_samples, num_features)
labels = (torch.rand(num_samples) > 0.5).long()  # 0과 1의 이진 레이블 생성

test_features = torch.randn(num_test_samples, num_features)
test_labels = (torch.rand(num_test_samples) > 0.5).long()  # 0과 1의 이진 레이블 생성


# 2. 학습/테스트 데이터셋과 데이터 로더 설정
dataset = TensorDataset(features, labels) # 데이터셋
train_loader = DataLoader(dataset, batch_size=64, shuffle=True) # 데이터 로더

test_dataset = TensorDataset(test_features, test_labels)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # 일반적으로 테스트 데이터를 섞을 필요는 없음
```

- 어떻게 모델을 정의하고, 모델, 손실함수, 옵티마이저를 설정하는가


```python
# 3. 신경망 모델 정의
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(num_features, 50)  # 입력층
        self.fc2 = nn.Linear(50, 20)            # 은닉층
        self.fc3 = nn.Linear(20, 2)             # 출력층

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

    
# 4. 모델, 손실 함수, 옵티마이저 초기화
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

- 어떻게 모델을 학습시키는가. 이 형식 자체는 외우는게 빠를듯


```python
num_epochs = 10
model.train() # 모델을 학습 모드로 설정

for epoch in range(num_epochs):
    for inputs, targets in train_loader: # 데이터 로더
        optimizer.zero_grad()  # 기울기 초기화
        output = model(inputs)    # 순전파
        loss = criterion(output, targets)  # 손실 계산
        loss.backward()         # 역전파
        optimizer.step()        # 옵티마이저로 파라미터 업데이트

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
```

    Epoch 1, Loss: 0.6854779720306396
    Epoch 2, Loss: 0.6852638125419617
    Epoch 3, Loss: 0.6905092000961304
    Epoch 4, Loss: 0.6679483652114868
    Epoch 5, Loss: 0.6705566048622131
    Epoch 6, Loss: 0.6738013029098511
    Epoch 7, Loss: 0.6612289547920227
    Epoch 8, Loss: 0.6761480569839478
    Epoch 9, Loss: 0.677962601184845
    Epoch 10, Loss: 0.6728285551071167
    

## 2. 실제 딥러닝 모델 평가 과정
- 손실 값을 바탕으로 모델이 잘 학습되었는지 확인.

모델 평가 과정
1. 평가 모드 설정 = model.eval()를 통해 학습 모드에서 평가 모드로 변경
2. gradient 계산 중지 = torch.no_grad() 옵션으로 계산 중지
3. 테스트 데이터로 예측
4. 성능 측정


```python
# 1. 모델을 평가 모드로 설정
model.eval()  

total = 0
correct = 0

# 2. 평가 중에는 그래디언트를 계산하지 않음
with torch.no_grad():  
    for inputs, targets in test_loader: # 테스트 데이터
        output = model(inputs) # 3. 순전파
        _, predicted = torch.max(output.data, 1)  # 가장 높은 점수를 받은 클래스 선택
        total += targets.size(0)  # 전체 샘플 수
        correct += (predicted == targets).sum().item()  # 4. 성능 측정 (정확히 예측된 샘플 수)

accuracy = 100 * correct / total  # 정확도 계산
print(f'Test Accuracy: {accuracy:.2f}%')
```

    Test Accuracy: 47.50%
    
