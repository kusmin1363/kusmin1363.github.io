---
layout: single
title:  "DL-8. RNN"
---

# 1. RNN이란?


## 1. RNN이 등장한 이유
- Sequence data를 올바르게 처리하기 위함.
- 배열된 순서 자체도 하나의 정보로 작용하기 때문에, 이를 기억하고 활용할 수 있는 구조가 필요해짐.

## 2. RNN 구조
- Recurrent Neural Network, 네트워크의 출력이 다시 네트워크의 입력으로 사용되는게 핵심임.
- 단순히 현재의 입력으로 예측값을 구하는게 아니라, 이전 시간대의 은닉 상태도 같이 사용함.
![png](/assets/images/DL-8-1.png)

- 또한 모든 시간대에서 같은 가중치를 사용하기 때문에, 전반적인 가중치 사용량이 적어서 효율적임

## 3. RNN이 Sequence data를 처리하는 유형
1. Many to One = 여러 시퀀스에서 단일 출력 결과 생성  
    ex) 문장 내 감정 분석, 문장 내 특정 단어 사용 여부
2. Many to Many = 두 시퀀스 간의 직접적인 매핑  
    ex) 실시간 음성 분석
3. One to Many = 하나의 초기벡터에서 연속적인 데이터 시퀀스 생성  
    ex) 텍스트 생성
4. Encoder-Decoder = 2번과 비슷한 경우지만, 입력 시퀀스와 출력 시퀀스의 길이가 다른 상황  
    ex) 기계 번역

# 2. Pytorch로 RNNcell 구현
![png](/assets/images/DL-8-2.png)

```python
import torch
import torch.nn as nn
#시드 고정
torch.manual_seed(42)
```




    <torch._C.Generator at 0x255c97fe9d0>



## 입력 시퀀스 데이터($X_t$) 생성


```python
d = 4  # 입력 x_t (1차원 벡터)의 크기
x_t = torch.randn(d)  # 현재 시점 t의 입력
display(f"x_t 출력 결과 : {x_t}")
display(f"x_t 차원 : {x_t.dim()}")
display(f"x_t 형태 : {x_t.shape}")
```


    'x_t 출력 결과 : tensor([-1.1229, -0.1863,  2.2082, -0.6380])'



    'x_t 차원 : 1'



    'x_t 형태 : torch.Size([4])'


## 은닉 상태($h_{t-1}$) 설정


```python
# 은닉 상태의 크기를 설정합니다.
Dh = 6  

# 이전 타임 스텝의 은닉 상태를 초기화
h_t_minus_1 = torch.zeros(1, Dh)  

display(f"이전 스텝 은닉상태 (h_t_minus_1) : {h_t_minus_1}")
display(f"이전 스텝 은닉상태 (h_t_minus_1) 차원 : {h_t_minus_1.dim()}")
display(f"이전 스텝 은닉상태 (h_t_minus_1) 형태 : {h_t_minus_1.shape}") 
```


    '이전 스텝 은닉상태 (h_t_minus_1) : tensor([[0., 0., 0., 0., 0., 0.]])'



    '이전 스텝 은닉상태 (h_t_minus_1) 차원 : 2'



    '이전 스텝 은닉상태 (h_t_minus_1) 형태 : torch.Size([1, 6])'


## 가중치($W_x$) 초기화


```python
Wx = torch.randn(Dh, d)
display("입력 가중치 행렬 : ")
display(Wx)
display(f"입력 가중치 행렬 차원 : {Wx.dim()}")
display(f"입력 가중치 행렬 형태 : {Wx.shape}")
```


    '입력 가중치 행렬 : '



    tensor([[-2.5095,  0.4880,  0.7846,  0.0286],
            [ 0.6408,  0.5832,  1.0669, -0.4502],
            [ 1.0311, -0.7048,  1.0131, -0.3308],
            [ 0.5177,  0.3878, -0.5797, -0.1691],
            [-0.5733,  0.5069, -0.4752, -0.4920],
            [ 0.2704, -0.5628,  0.6793,  0.4405]])



    '입력 가중치 행렬 차원 : 2'



    '입력 가중치 행렬 형태 : torch.Size([6, 4])'


## 가중치($W_h$) 초기화


```python
Wh = torch.randn(Dh,Dh)
display("은닉 상태 가중치 행렬 : ")
display(Wh)
display(f"입력 가중치 행렬 차원 : {Wh.dim()}")
display(f"입력 가중치 행렬 형태 : {Wh.shape}")
```


    '은닉 상태 가중치 행렬 : '



    tensor([[-0.3609, -0.0606,  0.0733,  0.8187,  1.4805,  0.3449],
            [-1.4241, -0.1163,  0.2176, -0.0467, -1.4335, -0.5665],
            [-0.4253,  0.2625, -1.4391,  0.5214,  1.0414, -0.3997],
            [-2.2933,  0.4976, -1.2956,  0.0503, -0.5855, -0.3900],
            [ 0.0358,  0.1206, -0.8057, -0.2076, -1.1586, -0.9637],
            [-0.3750,  0.8033, -0.5188, -1.5013, -1.9267,  0.1279]])



    '입력 가중치 행렬 차원 : 2'



    '입력 가중치 행렬 형태 : torch.Size([6, 6])'


## bias($b$) 초기화


```python
b = torch.randn(Dh)  # 편향

display("편향 벡터 : ")
display(b)
display(f"편향 차원 : {b.dim()}")
display(f"편향 형태 : {b.shape}")
```


    '편향 벡터 : '



    tensor([ 0.7764, -0.3029, -1.2753, -0.4758,  2.3839,  0.9157])



    '편향 차원 : 1'



    '편향 형태 : torch.Size([6])'


## 현 시점 은닉상태($h_t$) 계산
![png](/assets/images/DL-8-3.png)


```python
h_t = torch.tanh(torch.matmul(x_t.unsqueeze(0), Wx.t()) + torch.matmul(h_t_minus_1, Wh.t()) + b.unsqueeze(0))
h_t
```




    tensor([[ 0.9999,  0.9073,  0.1455, -0.9802,  0.9756,  0.9592]])



## 출력 가중치($W_y$)와 출력($Y_t$) 초기화 및 계산


```python
Dy = 2 
Wy = torch.randn(Dh, Dy)  # 출력 가중치
y_t = torch.sigmoid(torch.matmul(h_t, Wy))
print(f"최종 출력 : {y_t}")
print(f"최종 출력 형태 : {y_t.shape}")
```

    최종 출력 : tensor([[0.2330, 0.2465]])
    최종 출력 형태 : torch.Size([1, 2])
    

# 3. 클래스 모듈로 RNN 만들기
- CNN이나 딥러닝에서 하던것 처럼 모듈 상속받아서 설정해주면 됨


```python
import torch
import torch.nn as nn

class MyRNNcell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(MyRNNcell, self).__init__()
        self.hidden_size = hidden_size
        
        #nn.Parameter로 설정해줘서 Wx,Wh,b가 자동으로 그래디언트를 계산하고 업데이트하도록 설정
        self.Wx = nn.Parameter(torch.randn(hidden_size, input_size))
        self.Wh = nn.Parameter(torch.randn(hidden_size, hidden_size))
        self.b = nn.Parameter(torch.randn(hidden_size))

    def forward(self, x, hidden):
        # 행렬 곱을 위해서 Wx와 Wh의 Transpose를 취한후 계산
        hidden = torch.tanh(torch.matmul(x,self.Wx.t()) + torch.matmul(hidden, self.Wh.t()) + self.b)
        return hidden

## 확인하기
model_rnn = MyRNNcell(input_size=4, hidden_size=6)
model_rnn
```




    MyRNNcell()




```python
torch.manual_seed(42)

# 랜덤 입력 데이터 생성
x_t = torch.randn(1, 4)  # Input size = 4

# 초기 은닉 상태 생성
initial_hidden = torch.randn(1, 6)  # Hidden size = 6

# 모델 실행
new_h_t = model_rnn(x_t, initial_hidden)
print("새로운 은닉 상태:", new_h_t)
```

    새로운 은닉 상태: tensor([[ 0.9910, -0.9999, -0.9884,  0.9980, -0.8541,  0.9734]],
           grad_fn=<TanhBackward0>)
    

## 초기 은닉 상태도 생성해주는 클래스


```python
import torch
import torch.nn as nn

class MyRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyRNN, self).__init__()
        self.hidden_size = hidden_size
        #MyRNNcell을 생성함으로써, 자동으로 self.rnn_cell에 self.Wx, self.Wh, self.b가 생성됨
        self.rnn_cell = MyRNNcell(input_size, hidden_size)
        self.Wy = nn.Parameter(torch.randn(output_size, hidden_size)) 

    #첫번째 hidden_state를 생성해냄
    def init_hidden(self, batch_size=1):
        return torch.zeros(batch_size, self.hidden_size)
    
    def forward(self, x):
        #첫번째 hidden_state 생성
        h_t = self.init_hidden(x.size(0))
        
        for i in range(x.size(1)):  # Iterate over sequence
            # [0.1, 0.2, 0.3, 0.4] 이렇게가 x[:,i]로 들어간다.
            h_t = self.rnn_cell(x[:,i], h_t)
        
        output = torch.sigmoid(torch.matmul(h_t, self.Wy.t()))  # Sigmoid activation
        return output, h_t

sequence_data = torch.tensor([
    [0.1, 0.2, 0.3, 0.4],
    [0.2, 0.3, 0.4, 0.5],
    [0.3, 0.4, 0.5, 0.6],
    [0.4, 0.5, 0.6, 0.7],
    [0.5, 0.6, 0.7, 0.8],
])

# MyRNN 모델 인스턴스화
input_size = 4  # 입력 피처의 개수
hidden_size = 6  # 은닉 상태의 크기
output_size = 1  # 출력 크기

model_myrnn1 = MyRNN(input_size, hidden_size, output_size)

#sequence_data에 차원을 하나 더 추가해서, 1*4*4인채로 x에 들어가짐
output, ht = model_myrnn1(sequence_data.unsqueeze(0))
print("입력 시퀀스:", sequence_data)
print(f"모델 출력:, {output} {ht}")

```

    입력 시퀀스: tensor([[0.1000, 0.2000, 0.3000, 0.4000],
            [0.2000, 0.3000, 0.4000, 0.5000],
            [0.3000, 0.4000, 0.5000, 0.6000],
            [0.4000, 0.5000, 0.6000, 0.7000],
            [0.5000, 0.6000, 0.7000, 0.8000]])
    모델 출력:, tensor([[0.8964]], grad_fn=<SigmoidBackward0>) tensor([[0.9986, 0.9924, 0.7587, 0.3825, 0.9308, 0.2603]],
           grad_fn=<TanhBackward0>)
    

![png](/assets/images/DL-8-4.png)
