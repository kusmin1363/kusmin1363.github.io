---
layout: single
title:  "1일차. 연습용 파일 올리기"
---

```python

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
import random

def load_data(filename):
    data = np.loadtxt(filename)
    X = data[:, :-1]  # 마지막 열을 제외한 나머지가 X
    y = data[:, -1]   # 마지막 열이 y (레이블)
    return X, y

#GMM 클래스 정의
class GMM:
    def __init__(self, X, k, max_iter=200, tol=0.001, reg_covar=1e-6, noise_std=0.1):

        self.k = k
        self.max_iter = max_iter
        self.tol = tol
        self.X = X
        self.reg_covar = reg_covar
        self.noise_std = noise_std
        self.means = []
        self.covs = []

        #weights 초기화 = 각 weight의 값을 1/k(component의 개수)로 설정
        self.weights = np.array([1/k]*k)

        #mean 초기화 = 데이터 중에서 random sample을 선별해서 초기 mean으로  설정
        random.seed(100)
        idx = random.sample(range(X.shape[0]), k)
        self.means = X[idx,:]
 
        #covarinace 초기화 = 행렬을 생성해, 행렬의 원소에 랜덤 값을 집어넣음
        np.random.seed(100)
        cov_list = []

        for i in range(k):
            arr = np.random.rand(X.shape[1]**2)+0.1
            temp_mat = np.triu(arr.reshape(X.shape[1], X.shape[1]))
            cov_elem = temp_mat.dot(temp_mat.T)
            cov_list.append(cov_elem)
        
        self.covs = np.array(cov_list)

    def EM(self, X):
        # EM 알고리즘 실행
        self._em_algorithm(X)

    def _em_algorithm(self, X):
        log_likelihood = 0  # 초기화
        for i in range(self.max_iter): 
            # w = E_step을 통해 얻은 w값(특정 클러스터 k애 들어갈 비율(확률))
            w = self._e_step(X)
            self._m_step(X, w)

            new_log_likelihood = np.sum(np.log(np.sum(w, axis=1)+ 1e-10) )
            if log_likelihood != 0 and np.abs(new_log_likelihood - log_likelihood) < self.tol:
                break
            log_likelihood = new_log_likelihood

    def _e_step(self, X):    
        w = np.zeros((X.shape[0], self.k))
    
        N, D = X.shape  # 데이터의 샘플 수와 차원
        for k in range(self.k):
            det_sigma = np.linalg.det(self.covs[k])
            inv_sigma = np.linalg.inv(self.covs[k])

        # 데이터와 평균의 차이
            diff = X - self.means[k]  # [N, D]

        # Gaussian 지수 부분
            exponent = -0.5 * np.einsum('ij,jk,ik->i', diff, inv_sigma, diff)
            gaussian_numerator = np.exp(exponent)

        # Gaussian 분모 부분
            gaussian_denominator = np.sqrt(((2 * np.pi) ** D) * det_sigma)

        # w 계산
            w[:, k] = self.weights[k] * (gaussian_numerator / gaussian_denominator) 

    # w_sum 계산
        w_sum = np.sum(w, axis=1, keepdims=True) 
        w_sum[w_sum == 0] = 1e-10  # 혹시나 0으로 나누는 것 방지

    # w을 정규화
        w /= w_sum
        return w
    
    def _m_step(self, X, w):
        for k in range(self.k):
            Nk = np.sum(w[:, k], axis=0)
            
            # 평균 최신화
            self.means[k] = np.sum(w[:, k][:, np.newaxis] * X, axis=0) / Nk

            # 공분산 최신화
            diff = X - self.means[k]
            self.covs[k] = (1 / Nk) * (w[:, k][:, np.newaxis] * diff).T @ diff + self.reg_covar * np.eye(X.shape[1])

            # 혼합 계수 최신화
            self.weights[k] = Nk / X.shape[0]

    def _gaussian_pdf(self, X, mean, cov): #log likelihood에 자주 사용해서 미리 함수화
        d = X.shape[1]
        coeff = 1 / np.sqrt((2 * np.pi) ** d * np.linalg.det(cov))
        exponent = -0.5 * np.sum((X - mean) @ np.linalg.inv(cov) * (X - mean), axis=1)
        return coeff * np.exp(exponent)

    def predict(self, X): #데이터 X가 들어왔을 때 어떤 클래스로 예측되는가에 대한 함수
        w = self._e_step(X)
        return np.argmax(w, axis=1)  # 클래스 0 또는 1로 변환


# EM 알고리즘을 통한 GMM 기반 이진 분류
def em_gmm_classification(X_train, y_train, X_test, num_cluster):
    
    # 클래스별로 데이터를 나눔
    X_train_class0 = X_train[y_train == 0]
    X_train_class1 = X_train[y_train == 1]
    
    #각 클래스에 대해서 GMM class 생성.
    gmm0 = GMM(X_train_class0, k=num_cluster)
    gmm1 = GMM(X_train_class1, k=num_cluster)
    # 각 클래스에 대해 GMM 학습
    gmm0.EM(X_train_class0)  # 클래스 0에 대한 GMM
    gmm0_means = gmm0.means.copy()
    gmm0_covs = gmm0.covs.copy()
    gmm0_weights = gmm0.weights.copy()

    gmm1.EM(X_train_class1)  # 클래스 1에 대한 GMM
    gmm1_means = gmm1.means.copy() 
    gmm1_covs = gmm1.covs.copy()
    gmm1_weights = gmm1.weights.copy()

    # 테스트 데이터에서 각 클래스의 가능도 계산
    log_likelihood_class0 = np.sum([gmm0_weights[k] * gmm0._gaussian_pdf(X_test, gmm0_means[k], gmm0_covs[k]) for k in range(num_cluster)], axis=0)
    log_likelihood_class1 = np.sum([gmm1_weights[k] * gmm1._gaussian_pdf(X_test, gmm1_means[k], gmm1_covs[k]) for k in range(num_cluster)], axis=0)

    # 가능도가 더 높은 클래스로 분류
    y_pred = np.where(log_likelihood_class1 > log_likelihood_class0, 1, 0)
    return y_pred

# K-Fold 라이브러리를 이용해서
def k_fold(X, y, max_components=5, k_folds=5):
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=0)
    error_rates = []

    for n_components in range(1, max_components + 1):
        fold_accuracies = []

        for train_idx, test_idx in kf.split(X):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            y_pred = em_gmm_classification(X_train, y_train, X_test, n_components)
            accuracy = accuracy_score(y_test, y_pred) 
            fold_accuracies.append(accuracy)

        # 각 구성 요소 개수에 대한 평균 정확도
        mean_accuracy = np.mean(fold_accuracies)
        error_rates.append(1 - mean_accuracy)  # 오류율 계산

    # 최적의 구성 요소 개수 선택
    optimal_components = np.argmin(error_rates) + 1
    print("error rate", error_rates)
    return optimal_components, error_rates

# 오류 곡선 그래프 그리기
def plot_error_curve(error_rates):
    plt.plot(range(1, len(error_rates) + 1), error_rates, marker='o')
    plt.title('Error Rate vs Number of GMM Components')
    plt.xlabel('Number of GMM Components')
    plt.ylabel('Error Rate')
    plt.grid(True)
    plt.show()

# 메인 실행
if __name__ == "__main__":
    # train.txt와 test.txt에서 데이터 로드
    X_train, y_train = load_data('train.txt')
    X_test, y_test = load_data('test.txt')

    # K-Fold 교차 검증을 통해 최적의 혼합 구성 요소 개수 찾기
    optimal_num, error_rates = k_fold(X_train, y_train, max_components=10, k_folds=5)
    
    # 오류 곡선 그래프 출력
    plot_error_curve(error_rates)
    
    # 최적의 구성 요소 개수를 출력
    print(f'Optimal number of GMM components: {optimal_num}')
    
    # 최적의 구성 요소 개수로 train 데이터를 학습하고 test 데이터에서 검증
    y_pred = em_gmm_classification(X_train, y_train, X_test, optimal_num)
    accuracy = accuracy_score(y_test, y_pred) 
    error_rates = 1 - accuracy  # 오류율 계산
    print(error_rates)

```
